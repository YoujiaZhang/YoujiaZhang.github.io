<!DOCTYPE html>
<html lang="en">
  <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>USD</title>
        <!-- Bootstrap -->
        <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" async></script>
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  </head>
    
  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>Optimized View and Geometry Distillation from Multi-view Diffuser</h2>
            <h4 style="color:#5a6268;">Arxiv 2023</h4>
            <hr>
            <h6> Youjia Zhang, 
                 Junqing Yu, 
                 Zikai Song, 
                <a href="https://weiyang-hust.github.io/" target="_blank">Wei Yang</a></h6>
              
            <p>Huazhong University of Science and Technology</p>

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5">
                      <a class="btn btn-large btn-light" href="https://arxiv.org/pdf/2312.06198.pdf" role="button"  target="_blank">
                          <i class="fa fa-file"></i> Paper </a> 
                  </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/zju3dv/enerf" role="button"  target="_blank">
                    <i class="fa fa-github-alt"></i> Code (Coming soon)</a> </p>
              </div> -->
<!--               <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/zju3dv/ENeRF" role="button" target="_blank">
                    <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/zju3dv/ENeRF/blob/master/docs/enerf_outdoor.md" role="button">
                    <i class="fa fa-database"></i> Data</a> </p>
              </div> -->
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://raw.githubusercontent.com/haotongl/imgbed/master/enerf/supp.pdf" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Supplementary</a> </p>
              </div> -->
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            <h6 style="color:#8899a5" width="80%">
                Our technique produces multi-view images and geometries that are comparable, sometimes superior particularly for irregular camera poses, when benchmarked against concurrent methodologies such as SyncDreamer and Wonder3D, without training on large-scale data</h6>
            
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="videos/teaser_video.mp4" type="video/mp4">
            </video>
              <!-- <br><br> -->
          <p class="text-left">Generating multi-view images from a single input view using image-conditioned diffusion models is a recent advancement and has shown considerable potential. However, issues such as the lack of consistency in synthesized views and over-smoothing in extracted geometry persist. Previous methods integrate multi-view consistency modules or impose additional supervisory to enhance view consistency while compromising on the flexibility of camera positioning and limiting the versatility of view synthesis. In this study, we consider the radiance field optimized during geometry extraction as a more rigid consistency prior, compared to volume and ray aggregation used in previous works. We further identify and rectify a critical bias in the traditional radiance field optimization process through score distillation from a multi-view diffuser. We introduce an Unbiased Score Distillation (USD) that utilizes unconditioned noises from a 2D diffusion model, greatly refining the radiance field fidelity. we leverage the rendered views from the optimized radiance field as the basis and develop a two-step specialization process of a 2D diffusion model, which is adept at conducting object-specific denoising and generating high-quality multi-view images. Finally, we recover faithful geometry and texture directly from the refined multi-view images. Empirical evaluations demonstrate that our optimized geometry and view distillation technique generates comparable results to the state-of-the-art models trained on extensive datasets, all while maintaining freedom in camera positioning. </p>
            
        <hr style="margin-top:10px">
        
        <img class="img-fluid" src="images/pipeline.jpg" alt="Overall pipeline of our approach">

        <h6 style="color:#8899a5"> Overall pipeline of our approach </h6>
            
        </div>
      </div>
    </div>
  </section>
  <br>
    

    <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Denoising with unconditional noise</h3>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="videos/unconditional noise.mp4" type="video/mp4">
            </video>
            <br>
            <h6 style="color:#8899a5">
                The unconditional noise predicted by Zero-1-to-3 model tends to be biased. The right subfigure shows the averaged difference between the predicted noise and the added noise. 
                We take the unconditional noise predicted by Zero-1-to-3 to remove noise from the noisy input and recover the original image. We can see that even though a very low level of noise has been added, the denoised result deviates from the original image largely. In contrast, if we use the unconditional noise predicted by Stable Diffusion for denoising, only subtle details change while the main structure and identity of ‘Mario’ are preserved.</h6>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- overview video -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Unbiased Score Distillation</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" width="70%" src="images/main idea(1).jpg" alt="Overall pipeline of our approach">
            <h6 style="color:#8899a5">We propose to remove the added noise Zero-1-to-3 term by setting λ=0, and use the unconditional noise predicted by Stable Diffusion to replace that of Zero-1-to-3.</h6>
            
        </div>
      </div>
    </div>
  </section>
  <br>
    
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with SDS</h3>
            <hr style="margin-top:0px">
            <video width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="videos/main idea(2).mp4" type="video/mp4">
            </video>
            
        </div>
      </div>
    </div>
  </section>
  <br>
    
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Two-stage Specified Diffusion</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" width="70%" src="images/dreambooth.jpg" alt="Overall pipeline of our approach">
            
        </div>
      </div>
    </div>
  </section>
  <br>
    
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>More results</h3>
            <hr style="margin-top:0px">
            
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="videos/more.mp4" type="video/mp4">
            </video>
            
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@article{zhang2023optimized,
  title={Optimized View and Geometry Distillation from Multi-view Diffuser},
  author={Zhang, Youjia and Yu, Junqing and Song, Zikai and Yang, Wei},
  journal={arXiv preprint arXiv:2312.06198},
  year={2023}
}</code></pre>
          <hr>
      </div>
    </div>
  </div>

  <footer class="text-center" style="margin-bottom:10px">
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
  </footer>
    
</html>
