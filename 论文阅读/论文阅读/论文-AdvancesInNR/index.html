

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/%E5%B0%8F%E9%BE%99%E7%8C%AB.jpg">
  <link rel="icon" href="/img/%E5%B0%8F%E9%BE%99%E7%8C%AB.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zhang Youjia">
  <meta name="keywords" content="个人博客,blog,学习,Study">
  
    <meta name="description" content="摘要合成逼真的图像和视频是计算机图形学的核心，也是几十年来研究的重点。传统上，场景的合成图像是通过光栅化或光线追踪等渲染算法生成的，这些算法将具体定义的几何和材料属性的表示作为输入。总的来说，这些输入定义了实际的场景和被渲染的内容，并被称为场景表示（一个场景由一个或多个物体组成）。场景表示的例子是带有伴随纹理的三角形网格（例如，由艺术家创建）、点云（例如，来自深度传感器）、体积网格（例如，来自CT">
  
  
  <title>论文 Advances in Neural Rendering - Yj-Zhang&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="/css/mac.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"youjiazhang.github.io","root":"/","version":"1.8.13","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"siteId=17279802","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Youjia-Zhang&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about">
                <i class="iconfont icon-home-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/%E8%B5%B7%E9%A3%8E%E4%BA%86.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="论文 Advances in Neural Rendering">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-03-24 10:21" pubdate>
        March 24, 2022 am
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      36k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      114 分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">论文 Advances in Neural Rendering</h1>
            
            <div class="markdown-body">
              <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>合成逼真的图像和视频是计算机图形学的核心，也是几十年来研究的重点。传统上，场景的合成图像是通过光栅化或光线追踪等渲染算法生成的，这些算法将具体定义的几何和材料属性的表示作为输入。总的来说，这些输入定义了实际的场景和被渲染的内容，并被称为场景表示（一个场景由一个或多个物体组成）。场景表示的例子是带有伴随纹理的三角形网格（例如，由艺术家创建）、点云（例如，来自深度传感器）、体积网格（例如，来自CT扫描）或隐含的表面函数（例如，截断的有符号距离场）。使用可微分的渲染损失从观测中重建这样的场景表示，被称为逆向图形或逆向渲染。神经渲染与此密切相关，它结合了经典计算机图形学和机器学习的思想，创造了从真实世界的观察中合成图像的算法。神经渲染是朝着合成照片般真实的图像和视频内容的目标的一个飞跃。近年来，我们看到了这一领域的巨大进步，有数百篇论文展示了将可学习的组件注入渲染管道的不同方法。这篇关于神经渲染进展的最新报告着重介绍了将经典的渲染原理与学习到的三维场景表征相结合的方法，现在通常被称为神经场景表征。这些方法的一个主要优点是，它们在设计上是三维一致的，能够实现诸如捕获场景的新视点合成的应用。除了处理静态场景的方法外，我们还包括用于对非刚性变形物体进行建模以及场景编辑和组成的神经场景表征。虽然这些方法大多是针对场景的，但我们也讨论了可以跨物体类别通用的技术，并可用于生成性任务。除了回顾这些最先进的方法外，我们还对当前文献中使用的基本概念和定义进行了概述。最后，我们讨论了开放的挑战和社会影响。</p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><p>合成可控的、照片般真实的图像和视频是计算机图形学的基本目标之一。在过去的几十年里，人们开发了一些方法和表现形式来模仿真实相机的图像形成模型，包括对复杂材料和全局照明的处理。这些方法基于物理定律，模拟了从光源到虚拟摄像机的光传输，以便进行合成。为此，在渲染过程中必须知道场景的所有物理参数。例如，这些参数包含关于场景几何和材料属性的信息，如反射率或不透明度。考虑到这些信息，现代光线追踪技术可以生成照片般真实的图像。除了基于物理学的渲染方法，还有多种技术可以接近真实世界的图像形成模型。这些方法基于数学近似（例如，表面的片状线性近似；即三角形网格）和启发式方法（例如，Phong 着色）来提高适用性（例如，用于实时应用）。虽然这些方法需要较少的参数来表示一个场景，但所实现的真实性也会降低。</p>
<p>虽然传统的计算机图形学允许我们生成高质量的可控场景图像，但场景的所有物理参数，例如相机参数、光照和物体的主体都需要作为输入提供。如果我们想生成一个真实世界场景的可控图像，我们就需要从现有的观测数据（如图像和视频）中估计这些物理属性。这种估计任务被称为逆向渲染，是非常具有挑战性的，尤其是当目标是照相式真实合成时。相比之下，神经渲染是一个快速兴起的领域，它可以紧凑地表示场景，并且可以通过利用神经网络从现有的观察中学习渲染（见图1）。神经渲染的主要思想是将经典（基于物理学的）计算机图形学的见解与深度学习的最新进展相结合。与经典计算机图形学类似，神经渲染的目标是以一种可控的方式生成照片般真实的图像（参见[TFT∗20]中对神经渲染的定义）。例如，这包括新颖的视点合成、重新打光、场景变形和合成。</p>
<p>早期的神经渲染方法（见[TFT∗20]）使用神经网络将场景参数转换为输出图像。场景参数要么直接作为一维输入，要么使用经典的计算机图形管道来生成二维输入。深度神经网络根据对真实世界场景的观察进行训练，并学会对这些场景进行建模和渲染。深度神经网络可以被看作是一个统一的函数近似器。具体来说，一个网络根据其输入参数、模型结构和可训练参数定义了一个函数系列。随机梯度下降法被用来从这个空间中找到最能解释训练集的函数，这是由训练损失衡量的。从这个角度看，神经梯度的目的是找到控制参数和相应的输出图像之间的映射，H和W是图像的高度和宽度。这可以被理解为一个复杂而具有挑战性的稀疏数据插值问题。因此，神经渲染，类似于经典的函数拟合，必须在欠拟合和过度拟合之间进行权衡，即很好地代表训练集与泛化到未观察到的输入。如果网络的表示能力不足，所得到的图像质量就会很低，例如，结果是模糊的。另一方面，如果表征能力很强，网络就会过度适应训练集，而在测试时不能泛化到未见过的输入。找到正确的网络结构本身就是一门艺术。在神经渲染的背景下，设计正确的物理动机的归纳偏置往往需要强大的图形背景。这些物理动机的归纳偏向作为正则器，确保发现的函数接近于三维空间和/或图像形成在我们的真实世界中的工作方式，从而导致在测试时更好的泛化。归纳偏见可以以不同的方式添加到网络中。例如，在所使用的层方面，在网络的哪一点上，以何种形式提供输入，甚至通过整合经典计算机图形学中的非训练性（但可区分的）组件。这方面的一个很好的例子是最近的神经渲染技术，它试图通过只学习三维场景的表示，并依靠计算机图形学的渲染功能来监督，从而将建模和渲染过程分离开来。例如，神经辐射场（NeRF）[MST∗20]使用多层感知器（MLP）来逼近三维场景的辐射和密度场。这种学习到的体积表征可以通过使用分析性可微分的重绘（即体积集成）从任何虚拟摄像机中绘制出来。在训练中，假设从几个摄像机的视角观察场景。通过从这些训练视点渲染估计的三维场景，并使渲染的图像和观察到的图像之间的差异最小化，对这些观察进行网络工作的训练。一旦训练完成，由神经网络近似的三维场景就可以从一个新的视角进行渲染，实现可控的合成。与使用神经网络学习渲染函数的方法相比[TFT∗20]，NeRF在方法中更明确地使用了来自计算机图形学的知识，由于（物理）归纳性的偏向，能够更好地概括新的视图：场景的密度和辐射度的中间三维结构化表示。因此，NeRF在三维空间中学习了有物理意义的颜色和密度值，物理学启发的光线投射和体积整合可以在新的视图中持续呈现。</p>
<p>所取得的质量以及该方法的简单性，导致了该领域的 “爆炸性 “发展。已经取得了一些进展，改善了适用性，实现了可控性，捕捉了动态变化的场景，以及训练和推理时间。在本报告中，我们介绍了该领域的这些新进展。为了促进对这些方法的深入理解，我们在第3节中通过详细描述不同的组件和设计选择来讨论神经渲染的基本原理。具体来说，我们澄清了当前文献中使用的不同场景表征的定义（表面和体积方法），并描述了使用深度神经网络对其进行近似的方法。我们还介绍了来自计算机图形学的基本渲染函数，这些函数被用来训练这些表征。由于神经渲染是一个发展非常迅速的领域，在许多不同的维度上都有显著的进展，我们对最近的方法及其应用领域进行了分类，以提供一个简洁的发展概况。基于这个分类法和不同的应用领域，我们在第4节介绍了最先进的方法。报告的最后，第5节讨论了开放性的挑战，第6节讨论了照片逼真的合成媒体的社会影响。</p>
<h1 id="2-Scope-of-This-STAR"><a href="#2-Scope-of-This-STAR" class="headerlink" title="2 Scope of This STAR"></a>2 Scope of This STAR</h1><p>在这份最新的报告中，我们着重介绍了先进的神经渲染方法，这些方法将经典的渲染与可学习的三维表征相结合（见图2）。底层的神经三维表征在设计上是三维一致的，能够控制不同的场景参数。在本报告中，我们对不同的场景表示进行了全面的概述，并对从经典渲染管道和机器学习中借来的组件的基本原理进行了分析。我们进一步关注使用神经辐射场[MST∗20]和体积化渲染的方法。然而，我们并不关注那些主要在二维屏幕空间中进行推理的神经渲染方法；我们参考了[TFT∗20]关于此类方法的讨论。我们也不包括用于光线追踪成像的神经超采样和去噪方法[CKS∗17, KBS15]。</p>
<h1 id="3-Fundamentals-of-Neural-Rendering"><a href="#3-Fundamentals-of-Neural-Rendering" class="headerlink" title="3 Fundamentals of Neural Rendering"></a>3 Fundamentals of Neural Rendering</h1><p>神经渲染，特别是三维神经渲染是基于计算机图形学的经典概念（见图2）。神经渲染管道学会了从现实世界的图像中渲染和/或表示一个场景，这些图像可以是无序的图像集，也可以是结构化的多视图图像或视频。它通过模仿相机捕捉场景的物理过程来实现这一目标。三维神经渲染的一个关键特性是，在训练过程中，相机捕捉过程（即投影和图像形成）与三维场景表示之间的脱钩。这种分离有几个优点，特别是在图像合成过程中，能带来高水平的三维一致性（例如，在新视角合成中）。为了将投影和其他物理过程与三维场景表示分离开来，三维神经渲染方法依赖于计算机图形学中已知的图像形成模型（例如光栅化、点拼接或体积整合）。这些模型是以物理学为基础的，特别是发射器的光线与场景以及相机本身的相互作用。这种光传输是用渲染方程[Kaj86]来表述的。</p>
<p>这些近似方法取决于所使用的场景表现，范围从经典的栅格化到路径追踪和体积整合。三维神经渲染就是利用了这些渲染方法。在下文中，我们将详细介绍常见的神经渲染方法中所使用的场景表示（第3.1节）和渲染方法（第3.2节）。请注意，为了从真实图像中学习，场景表示和渲染方法本身都必须是可分的（第3.3节）。</p>
<h2 id="3-1-Scene-Representations"><a href="#3-1-Scene-Representations" class="headerlink" title="3.1. Scene Representations"></a>3.1. Scene Representations</h2><p>几十年来，计算机图形学界一直在探索各种基元，包括点云、隐式和参数化表面、网格和体积（见图3）。虽然这些表现形式在计算机图形学领域有明确的定义，但在目前的神经渲染文献中，特别是关于隐式和显式表面表现和体积表现的文献中，常常会出现混淆。一般来说，体积表征可以代表表面，但反之则不行。体积表征可以存储体积属性，如密度、不透明度或占用率，但它们也可以存储多维特征，如颜色或辐射度。与体积表征相反，表面表征存储的是物体表面的属性。它们不能用来模拟体积物质，如烟雾（除非是一个粗略的近似）。对于表面和体积表征，都有连续和离散的部分（见图3）。连续表征对神经渲染方法特别有趣，因为它们可以提供分析梯度。</p>
<p>对于曲面的表示，有两种不同的方式来代表曲面–显性的或隐性的。在欧几里得空间中使用隐式曲面函数的曲面定义为：</p>
<p>一般来说，对于这三种场景表征，底层函数可以是任何能够近似地表达其内容的函数。对于像平面这样的简单表面，函数fimplicit , fexplicit可以是线性函数。为了处理更复杂的表面或体积，可以使用多项式（例如来自Taylor series）或多变量高斯。为了进一步提高表达能力，这些函数可以在空间上进行定位，然后组合成一个混合物，例如，多个高斯可以形成高斯混合物。径向基函数网络就是这样的混合模型，可以作为隐含面和体积函数的近似器[CBC∗01a]。请注意，这些径向基函数网络可以被解释为单层的神经网络。</p>
<p>由于神经网络，特别是多层感知器(MLPs)是通用的函数近似器，它们可以用来 “学习 “基础函数(fimplicit, fexplicit, fparametric, orfvol)。(与高斯混合物类似，多个局部的、较弱的MLP也可以组合成一个混合物，例如，[RPLG21]）。在神经渲染的背景下，使用神经网络来逼近表面或体积表示函数的场景表示被称为神经场景表示。请注意，表面和体积表征都可以被扩展到存储额外的信息，比如颜色或与视图相关的辐射度。</p>
<p>在下文中，我们将讨论不同的基于MLP的函数近似器，这些近似器为最近的神经表面和体积表征奠定了基础。</p>
<h3 id="3-1-1-Multi-Layer-Perceptron-as-a-Universal-FunctionApproximator"><a href="#3-1-1-Multi-Layer-Perceptron-as-a-Universal-FunctionApproximator" class="headerlink" title="3.1.1. Multi-Layer Perceptron as a Universal FunctionApproximator"></a>3.1.1. Multi-Layer Perceptron as a Universal FunctionApproximator</h3><p>多层感知器（MLPs）被认为是通用函数近似器[HSW89]。具体来说，我们使用MLPs来表示表面或体积的属性。多层感知器是一个传统的全连接的神经网络。在场景再现的背景下，MLP将一个坐标空间作为输入，并产生与该坐标相对应的一些值作为输出。这种类型的网络也被称为基于坐标的神经网络（由此产生的表示被称为基于坐标的场景表示）。请注意，输入的坐标空间可以与欧几里得空间对齐，但它也可以被嵌入到例如网格的uv空间中（产生一个神经参数化的表面）。</p>
<p>将基于ReLU的MLPs用于神经表示和渲染任务的一个关键发现是使用了位置编码。受自然语言处理中使用的位置编码的启发（例如，在变形金刚[VSP∗17]中），输入坐标使用一组基函数进行位置编码。这些基函数可以是固定的[MST∗20]，也可以是学习的[TSM∗20]。这些空间嵌入简化了MLP学习从一个位置到一个特定值的映射的任务，因为通过空间嵌入，输入空间被分割了。作为一个例子，NeRF[MST∗20]中使用的位置编码被定义为。</p>
<p>这里，x是输入坐标，p是一个控制使用频率的超参数（取决于目标信号的分辨率）。这种对输入坐标的 “软 “二进制编码使网络更容易获得更高频率的输入。</p>
<p>如上所述，基于MLP的函数近似器可以用来表示表面或体积（即fimplicit、fxplicit、fparametric或fvol），但它们也可以用来存储其他属性，如颜色。例如，有一些混合表示法，由经典的表面表示法如点云或网格组成，用MLP来存储表面外观（如texturefield [OMN∗19]）。</p>
<h3 id="3-1-2-Representing-Surfaces"><a href="#3-1-2-Representing-Surfaces" class="headerlink" title="3.1.2. Representing Surfaces"></a>3.1.2. Representing Surfaces</h3><p><strong>点云</strong>。点云是一组欧几里得空间的元素。一个连续的表面可以被点云离散化–点云的每个元素代表表面上的一个样本点（x,y,z）。对于每个点，可以存储额外的属性，如法线或颜色。以法线为特征的点云也被称为定向点云。除了可以被看作是无限小的表面补丁的简单点之外，还可以使用有半径的定向点云（代表一个位于底层表面的切平面上的二维盘）。这种代表被称为表面元素，别名surfels[PZvBG00]。在计算机图形学中，它们经常被用来渲染点云或模拟的部分。这种surfels的渲染被称为split-ting，最近的工作表明它是可分的[YSW∗19a]。使用这种可分的渲染管道，有可能直接回传到点云的位置以及伴随的特征（如半径或颜色）。在Neural Point-basedGraphics[ASK∗20a]和SynSin[WGSJ20]中，可学习的特征被附加到点上，可以存储关于实际表面的ap-pearance和形状的丰富信息。在ADOP[RFS21a]中，这些可学习的特征被一个MLP解释，它可以解释与视图相关的影响。</p>
<p>如上所述，点云是欧几里得空间的一组元素，因此，除了表面，它们也可以代表体积（例如，存储额外的不透明度或密度值）。对每个点使用阿拉迪乌斯自然会导致一个完整的基于球体的公式[LZ21]。</p>
<p><strong>网格</strong>。多边形网格表示一个表面的片状线性逼近。特别是，三角形和四边形网格在计算机图形中被用作表面的事实上的标准表示。图形管道和图形加速器（GPU）被优化为每秒处理和光栅化数十亿个三角形。大多数图形编辑工具都使用三角形网格，这使得这种表示方法对任何内容创建管道都很重要。为了与这些管道直接兼容，许多 “经典 “的逆向图形和神经渲染方法都使用这种基本的曲面表示。使用可区分的渲染器，顶点位置和顶点属性（如颜色）可以被优化，以再现图像。神经网络可以被训练来预测顶点位置，例如，预测动态变化的表面[BNT21]。代替使用顶点属性，在三角形内存储表面属性的常见策略是纹理图。二维纹理坐标被附加到网格的顶点上，这些顶点参考纹理图像中的某个位置。使用arycentric插值，可以计算三角形中任何一点的纹理坐标，并且可以使用双线性插值从纹理中检索属性。纹理的概念也被整合到了标准的图形管道中，还有一些额外的功能，如mip-mapping，这是正确处理纹理采样所需要的（比方说，采样定理）。延迟神经渲染[TZN19]，使用包含可学习的视图依赖特征的纹理，即所谓的神经纹理。具体来说，粗网被用作底层的三维表示，以栅格化神经质感的纹理。一个神经网络在图像空间解释这些栅格化的图像。请注意，该网络可以是一个像素级的MLP，然后神经纹理代表表面辐射率。</p>
<p>与使用离散纹理相比，可以使用连续纹理。纹理场[OMN∗19]的作者提出使用MLP预测每个表面点的颜色值。在神经反射场纹理（NeRF-Tex）[BGP∗21]中，NeRF[MST∗20]的思想与使用二维神经纹理和底层三维网格的思想相结合。NeRF-Tex以用户定义的参数为条件，控制外观，因此，可由艺术家来决定。</p>
<p><strong>隐式曲面</strong>。隐式曲面将曲面定义为一个函数的零点集，见公式3。最常用的隐式表面表示法是有符号距离函数（SDF）。这些SDF表示法被用于许多三维扫描技术，这些技术使用体积融合[CL96]来增量地重建静态[IKH∗11, NZIS13]或动态物体的表面[NFS15]。隐式表面表示法有很多优点，因为它们避免了定义网格模板的要求，因此能够表示未知拓扑结构的物体或在动态情况下改变拓扑结构。上面提到的体积融合方法使用离散的（截断的）符号距离函数，即使用一个包含符号距离值的三维网格。Hoppe等人[HDD∗92]提出了零散的线性函数来模拟输入表面点样本的符号距离函数。Carr等人的开创性工作[CBC∗01b]则使用了一个径向基函数网络。这个径向基函数网络工作表示一个连续的隐含曲面函数，可以看作是第一个 “神经 “隐含曲面表示。最近的神经隐含曲面表示是基于基于坐标的多层感知器(MLPs)，在第3.1.1节中有所介绍。它们同时在[PFS∗19, CZ19]中被提出用于形状建模，其中MLP架构被用来将连续坐标映射为符号距离值。由这种协同网络表示的信号的保真度，或神经隐含表示，主要受网络容量的限制。因此，与上述其他表征相比，隐含曲面在记忆效率方面具有潜在的优势，而且作为一种连续表征，它们在理论上可以以无限的分辨率来表示几何图形。 在最初的建议中，人们以广泛的热情进行了各种不同的改进，包括改进训练方案[XFYS20, DZW∗20, YAK∗20]。利用全局-局部背景[XWC∗19, EGO∗20]，采用特定的参数化[GCV∗19, DGY∗20, CTZ20, KJJ∗21, YRSH21]或空间分区[GCS∗20, TTG∗20, CLI∗20, TLY∗21, MLL∗21]。由于不需要预先定义网格模板或对象的拓扑结构，神经隐含曲面很适合于对不同拓扑结构的对象进行建模[PFS∗19, CZ19]。输出相对于输入坐标的分析梯度可以通过反向传播来计算。这使得有可能在梯度上实现正则化条款[GYH∗20]，以及其他以几何为动机的正则化器[GYH∗20, PFAK20, YAK∗20]。这些表述可以被扩展到也可以编码场景的辐射度[KJJ∗21, YTB∗21, SHN∗19]。这对神经渲染是很有用的，我们希望场景表示能够同时编码场景的几何和外观。</p>
<h3 id="3-1-3-Representing-Volumes"><a href="#3-1-3-Representing-Volumes" class="headerlink" title="3.1.3. Representing Volumes"></a>3.1.3. Representing Volumes</h3><p><strong>体素网格</strong>。作为R3中的像素等价物，体素通常被用来表示体积。它们可以存储几何体的占有率，或者存储具有体积效应的场景的密度值，例如透明度。此外，场景的外观可以被记录下来[GSHG98]。使用三线插值，这些体积属性可以在体素网格中的任何一点被访问。这种插值法特别适用于基于样本的渲染方法，如射线投射。虽然存储的属性可以有特殊的语义（如占用率），但这些属性也可以被学习。Sitzmann等人提出使用DeepVoxels[STH∗19]，其中特征被存储在一个体素网格中。在光线投射渲染过程中，使用深度神经网络对特征进行积累和解释。这些DeepVoxel可以被看作是体积神经纹理，它可以直接使用反向传播进行优化。虽然基于密集体素的回复查询速度很快，但它们的内存效率很低，而且3DCNN可能会在这些体积上操作，计算量很大。八叉树数据结构[LK10]可以用来以稀疏的方式重新表达体积。八叉树上的稀疏三维卷积[WGG∗17, ROUG17]可以帮助缓解一些问题，但是这些紧凑的数据结构不容易被即时更新。其他缓解密集体素网格记忆挑战的方法包括使用特定对象的形状模板[KTEM18]、多平面[ZTF∗18, MSOC∗19, FBD∗19, TS20, WPYS21]或多球体[BFO∗20, ALG∗20]图像，这些方法都是为了用稀疏的近似方式来表达体素网格。</p>
<p><strong>神经体积表征</strong>。不使用体素网格来存储特征或其他感兴趣的量，这些量也可以使用神经网络来定义，类似于神经隐含表面（见第3.1.2节）。MLP网络结构可用于参数化体积，可能比明确的体素网格更节省内存。但是，根据底层网络的大小，这些表示法的采样成本可能很高，因为对于每个采样，必须计算整个网络的前馈传递。大多数方法可以大致分为使用全局或局部网络[GCV∗19, GCS∗20, CZ19, MPJ∗19, al20, shn∗19, szw19, omn∗19, gyh∗20, ykm∗20, dnj20, smb∗20, nmog20, lgl∗20, jjhz20, lzp∗20, ksw20] 。 同时使用网格和神经网络工程的混合表征在计算能力和内存效率之间做了权衡[PNM∗20, JSM∗20, CLI∗20, MLL∗21]。与神经隐式曲面类似，神经体积表征允许计算分析梯度，这在[SMB∗20, TTG∗21, PSB∗21]中被用来定义ularization项。</p>
<p><em><strong>一般性评论</strong></em>。使用基于坐标的神经网络对场景进行体积建模（如NeRF），表面上看与使用坐标网络对曲面进行隐含建模（如neural implicit surfaces）相似。然而，类似于NeRF的体积代表并不一定是隐性的–因为网络工作的输出是密度和颜色，场景的几何参数是由网络明确地而不是隐性地确定的。尽管如此，在文献中，这些模型仍然被称为 “隐式”，也许是指场景的几何是由神经网络的权重 “隐式 “定义的（这与SDF文献中使用的 “隐式 “定义不同）。请注意，这是与深度学习和统计学界普遍使用的 “隐式 “定义不同的定义。”隐式 “通常是指其输出被隐式地定义为动态系统的固定点的模型，其梯度是用隐式函数定理来计算的[BKK19] 。</p>
<h2 id="3-2-Differentiable-Image-Formation"><a href="#3-2-Differentiable-Image-Formation" class="headerlink" title="3.2. Differentiable Image Formation"></a>3.2. Differentiable Image Formation</h2><p>前面几节中的场景表示使我们能够再现场景的三维几何和外观。作为下一步，我们将描述如何通过渲染从这种场景再现中生成图像。将三维场景渲染成二维图像平面的一般方法有两种：射线投射和Ras-terization，也见图4。场景的渲染图像可以通过定义场景中的相机来计算。大多数方法都使用针孔摄像机，所有摄像机的光线都会通过空间中的一个点（焦点）。对于一个给定的摄像机，从摄像机原点出发的光线可以投向场景，以计算出渲染的图像。</p>
<p><strong>射线投射</strong>。在针孔模型中，基本的截距定理可以用来描述3D中的一个点p∈R3是如何投射到图像平面中的正确位置q∈R2的。根据定义，它是一个非注入函数，很难反转–这使它成为三维重建问题的核心。</p>
<p>针孔模型在这个项目中只有一个参数矩阵：内在矩阵K包含了按像素大小归一化的焦距f=[αx,αy]，轴的倾斜度γ和中心点c=[cx,cy]。利用截距定理并假设同质坐标p′= [x,y,z,1]，我们发现投影坐标为q′= K-p′。</p>
<p>这假定投影的中心在坐标原点上，并且相机是轴对齐的。为了对任意的相机位置进行概括，可以使用一个外在矩阵R。这个4×4的均质矩阵E由以下部分组成</p>
<p>其中R是一个旋转矩阵，t是一个平移矢量，这样R -pw + t = pc，我们用pw来表示世界坐标中的一个点，用pc来表示相机坐标中的点。这种R和t的定义在计算机视觉中很常见（例如，由OpenCV使用），被称为 “世界到相机 “的映射，而在计算机图形中（例如，在OpenGL中），类似的反向 “相机到世界 “的映射更为普遍。假设采用 “世界到凸轮 “的惯例并使用同质坐标，我们可以将pw到qp的完整投影写成。</p>
<p>如果使用 “凸轮到世界 “的惯例，射线铸造是类似的方便。虽然这些方程由于深度的模糊性而是非主观的，但它们很适合于自动分化，并且可以在图像形成模型中进行端到端的优化。</p>
<p>为了对当前的相机进行正确建模，还有一个部件必须被考虑在内：镜头。撇开诸如景深或运动模糊等必须在图像形成过程中进行建模的影响不谈，它们在投影功能中增加了失真效应。不幸的是，没有一个简单的模型来捕捉所有不同的镜头效果。校准包，如OpenCV中的校准包，通常实现了有多达12个畸变参数的模型。它们是通过五度以下的多项式建模的，因此不是可逆的（相对于点投影而言，这是射线投影所要求的）。更现代的相机校准方法使用了更多的参数，并实现了更高的精度[SLPS20]，并且可以做到可逆和可微。</p>
<p><strong>栅格化</strong>。光线投射的一个替代方法是地理计量基元的栅格化。这种技术并不试图模仿真实世界的图像形成过程，而是利用物体的地理计量属性来快速创建图像。它主要用于网格，网格由一组顶点v和面f描述，连接三倍或四倍的顶点以定义面。一个基本的见解是，3D中的几何操作可以只对顶点进行操作：例如，我们可以使用相同的外在矩阵E来将每个点从世界坐标系转换到相机坐标系。在这种转换之后，视域之外的点或具有错误法线方向的点可以被剔除，以减少在接下来的步骤中要处理的点和面的数量。剩下的点投射到图像坐标上的位置又可以通过使用上述的内在矩阵K来找到，非常简单。脸部信息可用于插值脸部基元的深度，而最上面的脸部可存储在一个Z缓冲区中。</p>
<p>这种实现投影的方式通常比光线投射快：它主要是随着场景中可见顶点的数量而缩放，而光线投射则随着像素的数量和要相交的基元数量而缩放。然而，使用它很难捕捉到某些效果（例如，光照效果、阴影、再反射）。它可以通过 “软 “栅格化而变得可微分。例如，在[LLCL19, RRN∗20]中已经实现了这一点。</p>
<h3 id="3-2-1-Surface-Rendering"><a href="#3-2-1-Surface-Rendering" class="headerlink" title="3.2.1. Surface Rendering"></a>3.2.1. Surface Rendering</h3><p><strong>点云渲染</strong>。在计算机图形学文献中，点云渲染技术被广泛使用[KB04, SP04]。如前所述，点云是连续表面或体积的离散样本。点云渲染相当于从不规则分布的离散样本中重构连续信号，例如连续表面的外观，然后在每个像素点的图像空间中对重构的信号进行重新采样。</p>
<p>这个过程可以用两种不同的方式完成。第一种方法是基于经典的信号处理理论，可以看作是一种 “软 “的点拼接（类似于下面网格渲染部分的软Ras-terizer）。它首先使用连续的局部重建ker-nels r (-)构建连续信号，即f = ∑ fir (pi)。本质上，这种方法相当于将离散样本与一些局部确定性模糊核[LKL18, ID18, RROG18]相混合，例如EWA splat-ting[ZPVBG01, ZPVBG02]，它是一个空间变化的重建核，旨在最小化混叠。在神经渲染中，离散的样本可以存储一些可学习的特征。相应地，上述步骤有效地将各个特征投射并混合成一个二维特征图。如果这些特征具有预定的语义（如颜色、法线），可以使用固定的阴影函数或BRDF来生成最终的图像。如果这些特征是学习的神经描述符，则部署一个二维神经网络来将二维特征图转化为RGB图像。由于性能原因，它们在混合步骤中使用空间不变和各向同性的核。虽然这些简化的核子可能会导致渲染的假象，如孔洞、模糊的边缘和混叠，但这些假象可以在神经着色步骤中得到补偿。</p>
<p>除了软点拼接的方法外，我们可以使用OpenGL或DirectX的传统点渲染器。在这里，每个点被投射到一个像素（或一个小的像素区域），从而形成一个稀疏的特征图。我们可以使用深度神经网络直接在图像空间中重构信号[ASK∗20b]。请注意，这种天真的渲染方法并不提供相对于点位置p的梯度，只允许对（神经）特征进行差异化处理的函数。相比之下，软点拼接方法通过其构造核r (p)提供点位置梯度。</p>
<p>然而，即使在这种情况下，梯度也被限制在局部重建的支持空间内。[YSW∗19b]通过使用有限差分逼近梯度来解决这个问题，并成功地将渲染器应用于曲面变性、风格化和多视图形状重建。这个想法在[RFS21b]中被采用，以优化几何体和摄像机的姿势，共同进行新的视图合成。</p>
<p><strong>网格渲染</strong>。有许多通用的渲染器允许网格被栅格化或以其他可区分的方式进行渲染。在可分化的网格渲染器中，Loper和Black[LB14]开发了一个名为OpenDR的可分化渲染框架，该框架接近于初级渲染器，并通过自动分化计算梯度。Neuralmesh renderer (NMR) [KUH18]使用一个手工制作的可见度变化函数来接近光栅化操作的后向梯度。[LTJ18]提出了Paparazzi，这是一个使用图像滤波器进行网格几何处理的分析性差分渲染器。Petersen等人[PBDCO19]提出了Pix2Vex，一个通过附近三角形的软混合方案的C∞差异化渲染器，[LLCL19]引入了Soft Rasterizer，它渲染和聚合了网格三角形的概率图，允许从渲染的像素到闭塞的和远距离的顶点的渐变流。虽然大多数光栅器只支持基于直接光照的渲染，但[LHL∗21]也支持软阴影的可区分渲染。在基于物理的渲染领域，[LADL18a]和[ALKN19]引入了一个可区分的光线追踪器，以实现基于物理的渲染效果的可区分性，处理相机位置、照明和纹理。此外，Mitsuba 2 [NDVZJ19]和Taichi [HLA∗19, HAL∗20]是通用的基于物理的渲染器，通过自动分化支持可分化的网格渲染，以及其他许多图形学技术。</p>
<p><strong>神经隐式曲面渲染</strong>。当输入的观测值是二维图像的形式时，实现隐含曲面的网络不仅可以产生与几何有关的量，即有符号的距离值，还可以产生与外观有关的量。一个隐式可微分渲染器[SZW19, NMOG20, LZP∗20, LSCL19, YKM∗20, KJJ∗21, BKW21, TLY∗21]可以通过使用神经隐式函数的几何分支首先找到视线与曲面的交点，然后从外观分支获得该点的RGB值来实现。表面交点的搜索通常是基于球体追踪算法的某种变体[Har96]。球体追踪法从摄像机中心沿视线方向对三维空间进行迭代采样，直到到达表面。球体追踪是一种优化的射线行进方法，它通过在前一个位置采样的SDF值来调整步长，但这种迭代策略在计算上仍然很昂贵。Takikawa等人[TLY∗21]通过调整光线追踪算法以适应稀疏八叉树数据结构，改善了追踪性能。在[NMOG20,YKM∗20,KJJ∗21,BKW21]中，我们从二维图像中提取前景掩码，为几何分支提供额外的监督信号。最近，[OPG21]和[YGKL21b]通过将表面函数制定为体积渲染公式（下文介绍）来解决这个问题；另一方面，[ZYQ21]使用现成的深度估计方法来生成伪地面真实签名距离值，以协助几何分支的训练。</p>
<h3 id="3-2-2-Volumetric-Rendering"><a href="#3-2-2-Volumetric-Rendering" class="headerlink" title="3.2.2. Volumetric Rendering"></a>3.2.2. Volumetric Rendering</h3><p>体积渲染是以光线投射为基础的，并被证明在神经渲染中是有效的，特别是在从多视图输入数据中学习场景呈现方面。具体来说，场景被表示为一个连续的体积密度或占有率场，而不是一个硬表面的集合。</p>
<p>这意味着射线在空间的每一点上都有一些与场景内容相互作用的概率，而不是一个二进制的断面间事件。这种连续模型作为机器学习管道的微分渲染框架效果很好，这些机器学习管道在很大程度上依赖于存在良好的梯度进行优化。</p>
<p>尽管完全通用的容积率渲染确实考虑了 “散射 “事件，即光线可以从容积率粒子上反射出去[Jar08]，但我们将把这个总结限制在神经容积率渲染方法仅用于视图合成的基本模型上[LH96, Max95]，它仅考虑了 “发射 “和 “吸收 “事件，即光线被容积率粒子发射或阻挡。</p>
<p>给定一组像素坐标，我们可以使用前面描述的相机模型，计算出以原点p和方向ωo穿过三维空间的相应光线。沿着这条射线的入射光线可以用一个简单的发射/吸收模型定义为 </p>
<p>其中σ是某一点的体积密度，Le是某一点和方向的发射光，而透射率T是一个嵌套的积分表达式</p>
<p>密度表示一条光线在某一点与场景的体积 “介质 “相互作用的不同概率，而透射率则描述了当光线从p+tωo点向摄像机回流时，会有多少光线被衰减。</p>
<p>这些表达式只能对简单的密度和色域进行分析评估。在实践中，我们通常使用正交法对积分进行近似，其中σ和Le被假定为在一组N个区间{[ti-1,ti)}Ni=1内是片面恒定的，这些区间划分了射线的长度。</p>
<p>关于这个近似的完整推导，我们请读者参考Max和Chen[MC10]。请注意，当写成这种形式时，近似L的表达式完全对应于阿尔法将颜色L(i)e从后面到前面的组合[PD84]。</p>
<p>NeRF[MST∗20]和相关方法（例如，[MBRS∗21,NG21b,PCPMMN21,SDZ∗21,ZRSK20,NSP∗21]）使用可微分的体积渲染将场景表示投射到2D图像中。体积渲染需要沿射线处理多个样本，每个样本都需要在网络中进行完整的前向通过。最近的工作提出了增强的数据结构[YLT∗21, HSM∗21, GKJ∗21]、剪枝[LGL∗20]、重要性采样[NSP∗21]、快速集成[LMW21]和其他策略来加速渲染速度，尽管这些方法的训练时间仍然很慢。自适应坐标网络使用多分辨率网络架构加速训练，该架构在训练阶段通过以最佳和有效的方式分配可用的网络工作能力进行优化[MLL∗21]。</p>
<h2 id="3-3-Optimization"><a href="#3-3-Optimization" class="headerlink" title="3.3. Optimization"></a>3.3. Optimization</h2><p>训练神经网络的核心是一个非线性优化，其目的是应用训练集的约束条件，以获得一组神经网络的权重。因此，由神经网络近似的函数是适合于给定的训练数据的。通常，神经网络的优化是基于梯度的；更具体地说，利用SGD的变体，如Momentumor Adam [KB14]，其中梯度是通过反向传播算法的平均化而获得。在神经渲染的背景下，神经网络实现了三维场景的代表，而训练数据由场景的二维观察组成。使用神经场景代表的可微分渲染得到的渲染结果与使用各种损失函数的给定观察进行比较。这些重建损失可以用每个像素的L1或L2项来实现，但也可以使用感知[JAFF16]甚至是基于判别器的损失公式[GPAM∗14]。然而，关键是这些损失直接与相关的可区分的渲染公式相联系，以便更新场景的呈现，参见第3.1节。</p>
<h1 id="4-Applications"><a href="#4-Applications" class="headerlink" title="4. Applications"></a>4. Applications</h1><p>在这一节中，我们讨论了神经重塑的具体应用和基础神经场景表征。我们首先在第4.1节中讨论了对静态内容的新型视图合成的改进。然后，我们在第4.2节中概述了跨越物体和场景的通用方法。之后，第4.3节讨论了非静态、动态场景。接下来，我们在第4.4节中讨论了编辑和构成场景的问题。然后，我们在第4.5节中概述了重新打光和材料编辑。最后，我们在第4.6节中讨论了几个工程框架。我们还对每种应用的不同方法进行了分类。这些方法分别在表1、表2、表3、表4和表5中列出。</p>
<h2 id="4-1-Novel-View-Synthesis-of-Static-Content"><a href="#4-1-Novel-View-Synthesis-of-Static-Content" class="headerlink" title="4.1. Novel View Synthesis of Static Content"></a>4.1. Novel View Synthesis of Static Content</h2><p>新颖的视图合成是指从新的摄像机位置渲染一个给定场景的任务，给定的是一组图像和它们的摄像机姿态的输入。本节后面介绍的大多数应用都以某种方式对视图合成的任务进行了归纳：除了能够移动摄像机之外，它们还可能允许在场景中移动或变形物体，改变照明，等等。</p>
<p>视图合成方法是根据几个突出的标准来评估的。显然，输出的图像应该看起来尽可能的真实。然而，这并不是故事的全部–也许更重要的是多视角3D一致性。渲染的视频序列必须在摄像机在场景中移动时出现一致的3D内容，而不出现闪烁或扭曲。随着神经重绘领域的成熟，大多数方法都朝着产生一个固定的三维表示的方向发展，作为输出，可用于渲染新的二维视图，如在范围内解释。这种方法自动提供了一定程度的多视图一致性，而在历史上，如果过于依赖黑盒式二维卷积网络作为图像生成器或渲染器，则很难实现这种一致性。</p>
<h3 id="4-1-1-View-Synthesis-from-a-3D-Voxel-Grid-Representation"><a href="#4-1-1-View-Synthesis-from-a-3D-Voxel-Grid-Representation" class="headerlink" title="4.1.1. View Synthesis from a 3D Voxel Grid Representation"></a>4.1.1. View Synthesis from a 3D Voxel Grid Representation</h3><p>我们将简要回顾一下使用3D体素网格和体积渲染模型进行视图合成的最新历史。</p>
<p>DeepStereo[FNPS16]提出了第一个用于视图合成的端到端深度学习管道。这项工作包括许多现在已经变得很普遍的概念。一个卷积神经网络工作以平面扫描图（PSV）的形式出现在输入图像上，其中每个附近的输入都被重新投射到一组候选深度平面，要求网络简单地评估每个候选深度的每个像素的重新投射的匹配程度。CNN的输出被转换为一个使用softmax的深度概率分布，然后被用来结合一叠拟议的彩色图像（每个深度平面一个）。最后的损失只在渲染输出和保持的目标图像之间的像素级差异上执行，不需要中间的启发式损失。</p>
<p>DeepStereo的一个主要缺点是，它需要运行一个CNN来估计深度概率，并独立产生每个输出帧，从而导致运行时间缓慢，并缺乏多视角的三维一致性。Stereo Magnification[ZTF∗18]直接解决了这个问题，使用CNN将平面扫描的体积直接处理成输出的持久性三维体素网格表示，称为 “多平面图像”，或MPI。渲染新的视图只需要使用alpha合成来从一个新的位置渲染RGB-alphagrid。为了实现高质量的图像，立体放大技术严重扭曲了其3D网格的参数化，使其偏向于两个输入视图中的一个的参考框架。这大大降低了对密集网格的存储要求，但意味着新的视图只能在输入立体对的直接附近进行渲染。后来通过改进单个MPI的训练程序[STB∗19]，为网络提供两个以上的输入图像[FBD∗19]，或者将多个MPI结合起来，以代表一个场景[MSOC∗19]，来解决这一缺陷。</p>
<p>上述所有方法都使用前馈神经网络工作，从有限的输入图像集映射到输出图像或三维表示，并且必须在大量的输入/输出视图对的数据集中进行训练。相比之下，DeepVoxels[STH∗19]使用单一场景的图像，与学习的搜索器共同优化三维体素网格的特征，而不需要任何外部训练数据。同样，Neural Volumes [LSS∗19]优化了一个3DCNN，以产生一个多视图视频数据的单一场景的输出体积表示。这种单一场景的训练模式最近大为流行，利用了视图合成独特的 “自我监督 “方面：任何输入图像也可以通过渲染损失作为监督。与基于MPI的方法相比，DeepVoxels和Neural Volumes还使用了三维体素网格参数化，该网格不严重偏向于某一特定的观察方向，允许从任何方向观察重建的场景来渲染新的视图。</p>
<p>值得一提的是，一些计算机视觉研究人员主要关注三维形状重建（而不是现实的图像合成），他们在进行视图合成研究的同时，采用了阿尔法合成体表渲染模型[HRRR18, KHM17, TZEM17]；然而，这些结果受到三维CNN的内存限制的严重制约，无法产生超过1283分辨率的体素网格输出。</p>
<h3 id="4-1-2-View-Synthesis-from-a-Neural-Network-Representation"><a href="#4-1-2-View-Synthesis-from-a-Neural-Network-Representation" class="headerlink" title="4.1.2. View Synthesis from a Neural Network Representation"></a>4.1.2. View Synthesis from a Neural Network Representation</h3><p>为了解决体素网格的分辨率和内存限制，场景表示网络（SRNs）[SZW19]将基于球体追踪的神经渲染器与多层感知器（MLP）结合起来作为场景表示，主要关注于跨场景的泛化，以实现少数照片的重建。Differentiable Vol-umetric Rendering (DVR) [NMOG20]同样利用了Sur-face渲染方法，但证明了在特定场景中的过度拟合能够重建更复杂的外观和几何。</p>
<p>神经辐射场（NeRF[MST∗20]标志着基于MLP的场景表征在osingle-scene、逼真的新视图合成方面的应用取得了突破，见图5。NeRF没有采用基于表面的方法，而是直接应用第3.2.2节中描述的体积渲染模型，从输入位置和观察方向映射到输出体积密度和颜色的MLP合成图像。一组不同的MLP权重被优化以代表每一个新的输入场景，其依据是针对输入图像的像素级渲染损失。</p>
<p>这个整体框架与上一节中描述的工作有许多相似之处。然而，基于MLP的场景表示可以实现比离散的三维体积更高的分辨率，这是因为有效地微分压缩了sceneduring的优化。例如，一个能够渲染800×800分辨率的输出图像的NeRF表示只需要5MB的网络权重。相比之下，一个8003 RGBA体素网格会消耗接近2GB的存储空间。</p>
<p>这种能力可以归因于NeRF在通过MLP之前对输入的空间坐标使用了位置编码。与以前使用神经网络表示隐性表面[PFS∗19, CZ19]或体积[MON∗19]的工作相比，这使得NeRF的MLP可以表示更高频率的信号而不增加其容量（以网络权重的数量计算）。</p>
<p>从离散的三维网格转换到基于MLP的表示方法的主要缺点是渲染速度。与其直接查询一个简单的数据结构，现在计算空间中一个点的颜色和密度值需要评估整个神经网络（数十万次浮点运算）。在一个典型的桌面GPU上，一个标准的深度学习框架中的NeRF的实现需要几十秒来渲染一个高分辨率的图像。</p>
<h3 id="4-1-3-Improving-Rendering-Speed"><a href="#4-1-3-Improving-Rendering-Speed" class="headerlink" title="4.1.3. Improving Rendering Speed"></a>4.1.3. Improving Rendering Speed</h3><p>已经提出了几种不同的方法来加快基于MLP表示的体积计量渲染的速度。Neural SparseVoxel Fields [LGL∗20]在优化MLP的同时，建立并动态更新一个oc-tree结构，允许积极跳过空位和提前终止射线（当沿射线的传输距离接近零时）。KiloNeRF[RPLG21]将跳空和提前终止与密集的MLP三维网格结合起来，每个网格的权重数量比标准的NeRF网络小得多。</p>
<p>最近有三个同时进行的工作提出了在稀疏的三维网格上缓存NeRF MLP学到的各种数值的方法，一旦训练完成就可以进行实时渲染。SNeRG [HSM∗21]在稀疏的三维纹理图集中存储体积密度和一个小的空间变化特征向量，使用快速着色器沿射线合成这些值，并运行一个微小的MLP解码器，为每条射线生成与视图相关的颜色。FastNeRF [GKJ∗21]将体积密度与权重一起缓存起来，以结合一组学习的球形基础函数，在三维中的每一点产生随视图变化的颜色。PlenOctrees[YLT∗21]查询MLP以产生体积密度和球面单调系数的稀疏体素八叉树，并利用渲染损失进一步微调该八叉树表示，以提高其输出图像质量。</p>
<p>NeX-MPI[WPYS21]将多平面图像参数化与MLP场景表示结合起来，其视图依赖效应参数化为全局学习基函数的线性组合。由于该模型是在3DMPI坐标网格上直接监督的，一旦优化完成，该网格可以很容易地被缓存以实时渲染新的视图。</p>
<p>另一种加速渲染的方法是训练MLP表示本身，以有效地预先计算沿射线的部分或全部体积积分。AutoInt[LMW21]通过监督网络的梯度，使其行为类似于标准的NeRF MLP，来训练一个网络工作，以便沿着射线段 “自动整合 “输出颜色值。这使得渲染步骤可以将沿射线的积分分解成比标准正交估计少一个数量级的片段（少到2或4个采样点），以速度换取质量上的微小损失。Light Field Net-works[SRF∗21]更进一步，优化了MLP，对从输入光线到输出颜色（场景的光场）的映射进行了直接编码。这使得渲染时，每条光线只需对MLP进行一次评估，而基于体积和表面的渲染器则需要进行数百次评估，并能实现实时的新视角合成。这些方法在渲染速度和多视图一致性之间进行了权衡：将MLP表示重新参数化为射线而非三维点的函数，意味着从不同角度观看时，场景不再保证看起来一致。在这种情况下，多视角一致性必须通过监督来执行，要么提供大量的输入图像，要么通过在三维场景的数据集上泛化来学习这一特性。</p>
<h3 id="4-1-4-Miscellaneous-Improvements"><a href="#4-1-4-Miscellaneous-Improvements" class="headerlink" title="4.1.4. Miscellaneous Improvements"></a>4.1.4. Miscellaneous Improvements</h3><p>有很多论文都增强了渲染模型、超视距数据或体积MLP场景表示的鲁棒性。</p>
<p>深度监督。DONeRF[NSP∗21]训练了一个 “深度神谕 “网络来预测每条射线的采样位置，大大减少了通过NeRF MLP发送的采样数量，并允许交互式速率渲染。然而，这种方法对密集的深度图有超强的吸引力，而在真实数据中获得这种深度图是很困难的。深度监督的NeRF[DLZR21]使用稀疏的点云输出直接监督NeRF的输出深度（以每条射线的预期终止深度的形式），这是使用从运动中估计相机位置的副产品。</p>
<p>NerfingMVS[WLR∗21]使用多级管道进行深度监督，首先在稀疏的从运动中获得的深度估计上对单视角深度估计网络进行微调，然后使用重新产生的密集深度图来指导NeRF优化。</p>
<p><strong>优化相机姿态</strong>。NeRF–[WWX∗21]和自校准神经辐射场[JAC∗21]联合优化NeRF MLP和输入的摄像机姿势，绕过了对正向场景的运动结构预处理的需要。 Bundle-Adjusting Neural Radiance Fields (BARF) [LMTL21]通过对位置编码函数的每个频率分量应用从粗到细的退火计划，为联合重构和摄像机注册提供了更平滑的优化轨迹。然而，这些方法都不能为宽基线的360度拍摄从头开始优化姿势。GNeRF[MCL∗21]通过训练一组循环一致的网络（一个生成性的NeRF和一个姿势分类器）来实现这一目标，该网络从姿势映射到图像斑块，再返回到姿势，优化直到真实斑块的分类姿势与采样斑块的姿势相匹配为止。</p>
<p><strong>混合表面/体积表示法</strong>。Yariv等人的Implicit Differ-entiable Renderer(IDR)[YKM∗20]将类似DVR的隐含表面MLP与类似NeRF的视图相关分支结合起来，该分支将观察方向、隐含表面法线和三维表面点作为输入，并预测随视图变化的输出颜色。这项工作表明，将法线矢量作为颜色分支的输入有助于更有效地分解几何和外观的表示。它还表明，摄像机的姿势可以与形状表示一起联合优化，以便从小的误判误差中恢复。</p>
<p>UNISURF[OPG21]提出了一种混合的MLP表示法，将曲面和体积渲染结合在一起。为了渲染一条射线，UNISURF用寻根法得到一个 “表面 “交汇点，将体积视为一个占用场，然后只在该点周围的一个区间内分配体积渲染样本。这个区间的宽度在优化过程中单调地减少，允许早期迭代监督整个训练体积，而后期阶段则用紧密间隔的样本更有效地完善表面。Azinovic等人[AMBG∗21]建议使用SDF代表而不是体积密度来重建RGB-D数据的场景。他们将SDF值转换为密度，可以在NeRF公式中使用。NeuS[WLL∗21]将体积密度与一个有符号的距离场联系起来，并重新参数化透射函数，使其在这个SDF的零点交叉处达到最大斜率，允许对相应的表面进行无偏估计。VolSDF[YGKL21b]使用了一种从SDF到体积密度的交替映射，这使他们能够设计一种新的重采样策略，以实现体积渲染正交方程中近似不透明度的可证明的约束误差。在[LFS∗21]中，作者提出了一种称为MINE的方法，这是一种多平面图像（MPI）和NeRF之间的混合。他们能够从单色图像中重建密集的三维重建，他们在RealEstate10K、KITTI和Flow-ers光场上进行了演示。</p>
<p><strong>稳健性和质量</strong>。NeRF++[ZRSK20]提供了一个 “反转球体 “的空间参数化，可以让NeRF处理大尺度、无边界的三维场景。单位球体外的点被倒置回单位球体，并通过一个单独的MLP。</p>
<p>NeRF in the Wild [MBRS∗21]在MLP表示中增加了额外的模块，以考虑不同图像中不一致的照明和物体。他们将其强大的模型应用于Photo-Tourism数据集[SSS06]（由世界各地著名景点的互联网图像组成），并能够通过使用与每个输入图像相关的潜伏代码嵌入，去除人和汽车等瞬时物体，并捕获时间变化的外观。</p>
<p>MipNeRF[BMT∗21]修改了应用于三维点的位置编码，以纳入像素的足迹，见图6。通过对沿射线采样的每个正交段所对应的圆锥体进行位置编码，Mip-NeRF可以被训练成对多个不同尺度的场景表示进行编码（类似于二维文本图的mipmap），防止在从不同位置或分辨率渲染场景时出现混叠。</p>
<h2 id="4-2-Generalization-over-Object-and-Scene-Classes"><a href="#4-2-Generalization-over-Object-and-Scene-Classes" class="headerlink" title="4.2. Generalization over Object and Scene Classes"></a>4.2. Generalization over Object and Scene Classes</h2><p>虽然之前有大量的工作涉及到基于体素、基于网格或非三维结构的神经场景表征在多个场景和物体类别上的泛化，但我们将讨论的重点放在利用基于MLP的场景表征进行泛化的最新进展。在单一场景上过度拟合单一MLP的方法[MST∗20, YKM∗20]需要大量的图像观测，而跨场景表征泛化的核心目标是在给定少数或可能只有单一输入视图的情况下进行新的视图合成。在表2中，我们对所讨论的方法进行了概述，按照它们是否利用lo-cal或global conditioning、它们是否可以作为uncondi-tional generative model、它们利用哪种三维表征（体积、SDF或占位）、它们需要哪种训练数据以及如何进行推理（用编码器摊销、通过自动解码器框架或通过基于梯度的元学习）来分类。</p>
<p>我们可以区分两种跨场景泛化的关键方法。一种方法是采用基于图像的渲染方法[CW93, SK00]，将多个输入视图进行扭曲和混合以合成一个新的视角。在基于MLP的场景表示的背景下，这通常是通过局部调节来实现的，其中场景表示MLP的坐标输入与局部变化的特征向量相连接，存储在离散的场景表示中，如体素网格[PNM∗20]。PiFU[SHN∗19]使用一个图像编码器来计算输入图像的特征，并通过在图像平面上投射三维坐标来确定这些特征的三维MLP的条件–然而，PiFU并没有一个可区分的渲染器，因此需要地面真实的三维监督。PixelNeRF[YYTK21]（见图7）和Pixel-Aligned Avatars[RZS∗20]在体积渲染框架中利用了这种方法，这些特征被聚集在多个视图上，一个MLP产生颜色和密度场，像NeRF一样被渲染出来。当对多个场景进行训练时，他们学会了用于重建的场景先验，从而能够从几个视图中高保真地重建场景。PixelNeRF也可以对特定的物体类别进行训练，使物体实例从一个或多个摆放的图像中进行三维重建。GRF[TY20]使用类似的框架，有一个额外的注意力模块，对不同采样输入图像中的三维点的可见性进行推理。Stereo Radiance Fields [CBLPM21]同样从几个背景视图中提取特征，但利用了不同背景图像的成对特征之间的学习对应关系匹配来聚合不同背景图像的特征，而不是简单的平均聚合。最后，IBRNet[WWG∗21]和NeR-Former[RSH∗21]引入了跨射线样本的变换器网络，对可见性进行推理。</p>
<p>这种基于图像的方法的一个替代方案旨在学习一个单一的、全局的场景表示，而不是依赖图像或其他离散的空间数据结构。这是通过推断场景代表MLP的一组权重来实现的，它描述了整个场景，给定了一组观察结果。有一项工作是通过将场景编码为单一的、低维的潜伏代码来实现的，该代码随后被用来配置场景表示MLP。场景表示网络（SRNs）[SZW19]通过超网络将低维潜伏代码映射到MLP场景表示的参数上，并随后通过射线行进渲染产生的3D MLP。为了重新构建一个给定视图的实例，SRN优化lattent代码，使其渲染与输入视图相匹配。Differen-tiable Volumetric Rendering[NG20]同样使用表面渲染，但是通过分析计算其梯度并通过CNN编码器执行推理。光场网络[SRF∗21]利用低维潜伏代码直接对三维场景的四维光场进行参数化，实现了单次评估渲染。NeRF-VAE将NeRF嵌入到变异自动编码器中，同样在一个潜伏代码中表示整个场景，但学习一个生成模型，从而实现采样[KSZ∗21]。Sharf[RMBF21]使用一个类别中物体体素化形状的生成模型，这反过来又为更高分辨率的神经辐射场提供了条件，该神经辐射场使用体积渲染法进行渲染，以获得更高的新视点协同保真度。Fig-NeRF[XPMBB21]将一个物体类别建模为以潜伏代码为条件的模板形状，该模板经历了一个同样以潜伏变量为条件的去形成。这使得该网络能够将某些形状变化解释为更直观的变形。Fig-NeRF专注于从真实的物体扫描中检索物体类别，并建议使用学习背景模型将物体从其背景中分割出来。将场景表示为低维潜伏代码的另一种方法是通过基于梯度的元学习[SCT∗20]，在几个优化步骤中快速优化MLP场景表示的权重，这可用于从少数图像中快速重建神经辐射场[TMW∗21]。当在一个新的场景上训练时，预训练的模型融合得更快，而且与标准的神经辐射场训练相比，需要更少的视图。Portrait-NeRF[GSL∗20]提出了一种元学习方法，从一个人的单一正面图像中恢复NeRF。为了说明主体之间的姿势差异，它在一个姿势无关的典型参考框架中对三维肖像进行建模，该框架使用三维关键点对每个主体进行扭曲。Bergman等人[BKW21]利用基于梯度的元学习和图像特征的局部调节来快速恢复一个场景的新RF。</p>
<p>与推断低维潜伏代码相比，可以利用一种类似的方法来学习无条件的生成模型，而不是以对所寻求的三维场景的一组观察为条件。在这里，一个配备了神经渲染器的三维场景表示被嵌入到一个生成对抗网络中。我们不是从一组观察结果中推断出低维潜伏代码，而是定义潜伏代码的分布。在一个前向通道中，我们从该分布中抽取一个潜代码，在该潜代码上设置MLP场景表示的条件，并通过神经渲染器渲染一个图像。然后，该图像可用于对抗性损失。这样就可以学习一个关于三维场景的形状和外观的三维生成模型，只需给出二维图像。这种方法最早是通过体素网格[NPLT∗19]对三维场景表示进行参数化而提出的。GRAF[SLNG20]首次在这个框架中利用了有条件的NeRF，并在逼真度方面取得了显著的改善。Pi-GAN[CMK∗21]通过SIREN架构[SMB∗20]的基于FiLM的条件方案[PSDV∗18]进一步改进了这个架构。StyleN-eRF[Ano22]通过采用StyleGAN的条件机制，进一步提高了图像质量，并通过仅利用NeRF生成低分辨率的特征图和随后用精心设计的上采样网络进行上采样来提高运行时间（能够生成更高分辨率的图像），以确保多视图一致性。虽然这些方法不需要对每个三维场景进行更多的观察，因此也不需要真实的相机姿势，但它们仍然需要了解相机姿势的分布情况（例如，对于肖像图像，相机姿势的分布必须产生合理的肖像角度）。CAM-PARI[NG21a]通过联合学习摄像机姿势分布和生成模型来解决这一限制。GIRAFFE[NG21b]提出通过将一个场景参数化为几个前景（物体）NeRF和一个背景NeRF的组成，来学习由七个物体组成的场景的生成模型。每个NeRF的后验码被分别采样，然后由一个体积渲染器将它们组合成一个连贯的2D图像。</p>
<h2 id="4-3-Learning-to-Represent-and-Render-Non-static-Content"><a href="#4-3-Learning-to-Represent-and-Render-Non-static-Content" class="headerlink" title="4.3. Learning to Represent and Render Non-static Content"></a>4.3. Learning to Represent and Render Non-static Content</h2><p>虽然最初的神经辐射场[MST∗20]是用来表示静态场景和物体的，但也有一些方法可以补充处理动态变化的内容。在表3中，我们对所讨论的方法进行了概述。</p>
<p>这些方法可以分为时间变化的表现，允许对动态变化的场景进行新的视角合成，作为未修改的回放（例如，产生子弹时间效果），或者也可以对变形状态进行控制的技术，因此，允许对内容进行新的视角合成和编辑。神经辐射场的变形可以通过隐性或显性方式实现，见图8。</p>
<p>-隐性地，通过对变形状态的表征（例如，时间输入）来调节神经辐射场<br>-显式，通过使用一个单独的变形场，该变形场可以从变形空间映射到NeRF被嵌入的规范空间。</p>
<h3 id="4-3-1-Time-varying-Neural-Radiance-Fields"><a href="#4-3-1-Time-varying-Neural-Radiance-Fields" class="headerlink" title="4.3.1. Time-varying Neural Radiance Fields"></a>4.3.1. Time-varying Neural Radiance Fields</h3><p>时间变化的神经辐射场允许以新的视角播放视频，见图9。由于它们放弃了控制，这些方法不依赖于特定的运动模型，因此可以处理一般的物体和场景。</p>
<p>目前提出了几个针对非刚性场景的NeRF的扩展。我们首先讨论对变形进行隐式建模的方法[LNSW21, XHKK21, GSKH21, DZY∗21]。虽然最初的NeRF是静态的，只需要输入一个三维空间点，但它可以以一种简单的方式扩展为时变的：体积可以额外地以代表变形状态的矢量为条件。在目前的方法中，这种条件采取的形式是时间输入（可能是位置编码）或每个时间步长的自动解码潜伏代码[PSB∗21, TTG∗21, PSH∗21]。</p>
<p>由于在没有事先了解物体类型或三维形状的情况下处理非刚性场景是一个棘手的问题，这一类的方法采用了各种几何正则器，并以额外的数据模式为条件进行学习。为了鼓励不同时间段的重反射和不透明度的一致性，一些方法在时间上相邻的时间步骤之间学习场景流映射[LNSW21,XHKK21, GSKH21, DZY∗21]。由于这只限于小的时空邻域，无伪影的新视角合成主要表现在与输入相机轨迹相近的时空上。 场景流映射可以通过重建损失来训练，即把场景从其他时间步骤扭曲到当前时间步骤[LNSW21, DZY∗21]，通过鼓励估计的光流和场景流的二维投影之间的一致性[LNSW21,GSKH21]，或者通过跟踪三维中的反投影关键点[DZY∗21]。场景流经常受到额外的回归损失的约束[LNSW21, XHKK21, GSKH21, DZY∗21]，例如，为了鼓励空间或时间的平滑性或前向-后向周期的一致性。与其他提到的方法不同，Du等人[DZY∗21]的Neural Ra-diance FLow（NeRFlow）对具有无限小位移的deforma-tions进行建模，需要与Neural ODE[CRBD18]整合以获得偏移量。</p>
<p>此外，一些方法使用估计的深度图来监督几何估计[LNSW21, XHKK21, GSKH21, DZY∗21]。这种正则化的一个限制是，重建的准确性取决于单眼深度估计方法的准确性。因此，单眼深度估算方法的伪影在新的视图中是可以识别的[XHKK21]。</p>
<p>最后，静态背景经常被单独处理，使其能够利用单眼输入记录的多视图线索，跨越时间。为此，一些方法估计了不以变形为条件的第二个静态体积[LNSW21,GSKH21]，或者引入软正则化损失来约束静态场景内容[XHKK21]。Gao等人[GSKH21]是Xian等人工作[XHKK21]的后续，在二元分割掩码（模型的输入之一和用户提供的）的帮助下，对不包含移动和变形部分的观察进行静态NeRF训练。</p>
<p>Guo等人的方法的一个优点是，它在Yoon等人[YKG∗20]的挑战性数据集上产生了最准确的定量和定性结果（与Tretschk等人[TTG∗21]和Li等人[LNSW21]相比）。后者的数据集最初是为了从一组相当稀疏的动态场景的输入单眼视图中合成新的视图而引入的，摄像机的姿势有适度的变化。该方法的局限性包括对光流的强烈依赖和对任意非刚性变形的处理（与由独立刚性运动物体组成的场景不同）。</p>
<p>最后，NeRFlow可以用来对预先训练好的场景进行去噪和超分辨率的观察。研究者们提到的NeRFlow的局限性包括难以保留静态背景、处理复杂场景（非整体刚性变形和运动）以及在与输入的凸轮时代轨迹大相径庭的情况下渲染新的视图。</p>
<p>到目前为止讨论的方法都是通过对变形的场景表示进行调节来隐含地模拟变形。这使得变形的可控性变得繁琐和困难。其他作品则将变形与几何和外观分离开来：他们在静态典型场景的基础上将变形分解为一个单独的函数，这是实现可控性的关键一步。变形是通过向变形空间发射直光线，然后将其弯曲成典型场景来完成的，通常是通过使用基于坐标的MLP对直光线上的各点的偏移量进行回归，该MLP是以变形为条件的。这可以被认为是空间扭曲或场景流动。与隐式建模相比，这些方法通过静态典型场景的构架，在不同时间共享几何和外观信息，从而提供不会漂移的硬对应关系。由于这种硬约束，与隐式方法不同，目前具有显式变形的方法不能处理拓扑变化，只能在运动量明显小于隐式方法的场景中展示结果。</p>
<p>D-NeRF[PCPMMN21]使用一个非规则化的射线弯曲MLP来模拟单个或多个合成物体的变形，这些物体从背景中分离出来，由虚拟摄像机观察。它假定给定了一组预先定义的多视图图像，不过，在训练时，任何时候都只使用任意选择的单一视图进行监视。因此，D-NeRF可以被认为是多视角监督技术和真正的单眼方法之间的一个中间步骤。</p>
<p>一些作品展示了由移动的单眼相机观察的真实世界场景的结果。Park等人[PSB∗21]的DeformableNeRF的核心应用是创造Nerfies，即自由视角的自拍。Deformable NeRF对每个输入视图的变形和外观进行自动解码的潜伏代码。弯曲的光线是用一个 “尽可能刚性 “的项（也称为弹性能量项）来规范的，该项对偏离片状刚性场景配置的情况进行惩罚。因此，Deformable NeRF在有关节的场景（如握着网球拍的手）和人头（头部相对于躯干移动）等场景上效果良好。不过，由于正则器是软的，小的非刚性变形也能很好地处理（如微笑）。这项工作的另一个重要创新是采用从粗到细的方案，允许首先学习低频成分，避免因过度适应高频细节而造成局部最小值。</p>
<p>HyperNeRF[PSH∗21]是Deformable NeRF[PSB∗21]的一个扩展，使用了一个典型的超空间而不是一个单一的典型框架。这允许处理具有拓扑结构变化的场景，如开口和闭口。在HyperNeRF中，Deformable NeRF的弯曲网络（MLP）被一个环境切片表面网络（同样是一个MLP）所增强，该网络通过间接地将经典场景作为变形的条件，为每个输入的RGB视图选择一个经典的子空间。因此，它是一个混合体，结合了显性和隐性的变形建模，这使得它可以通过牺牲硬对应关系来处理拓扑变化。</p>
<p>非刚性NeRF（NR-NeRF）[TTG∗21]使用每个场景的典型体积、每个场景的活力标志（一个MLP）和每个帧的光线弯曲算子（一个MLP）对时间变化的场景外观建模。NR-NeRF显示，与[PSB∗21,XHKK21,LNSW21]相比，处理具有小的非刚性变形和运动的场景不需要额外的监督线索，如深度图或场景流。此外，观察到的变形被一个发散算子规律化了，该算子施加了一个体积保全的约束，并稳定了闭塞的区域，与监督的单眼输入视图相比。在这方面，它与Nerfies的弹性正则器有相似之处，即惩罚偏离片状刚性变形的行为。这种正则化使得新视角的摄像机轨迹有可能与输入的摄像机轨迹有明显的不同。虽然可控性仍然受到严重限制，但NR-NeRF展示了对学习到的变形场的几种简单编辑，如运动夸张或去除动态场景内容。</p>
<p>其他作品并不局限于单眼RGB输入视频的情况，而是考虑其他输入。</p>
<p>飞行时间辐射场（TöRF）方法[ALG∗21]重新取代了数据驱动的先验因素，用于从深度传感器中重建动态内容的深度图。与绝大多数计算机视觉作品不同的是，TöRF使用原始的ToF传感器测量值（所谓的相位），这在处理弱反射区域和现代深度传感器的其他限制（例如，有限的工作深度范围）时带来优势。在NeRF的学习过程中，对测量的场景深度的整合减少了对输入视图数量的要求，从而获得了清晰而详细的模型。</p>
<p>与NSFF[LNSW21]和时空神经辐照度场[XHKK21]相比，深度线索还能实现更高的精度。</p>
<p>神经三维视频合成[LSZ∗21]使用多视图RGB设置并隐含变形模型。该方法首先在关键帧上进行训练，从而利用了时空的平滑性。它还利用了摄像机保持静止的特点，并且通过对训练中的射线进行有偏见的采样，使场景内容先天性地保持静止。即使是小的动态内容，其结果也很清晰。</p>
<h3 id="4-3-2-Controllable-Dynamic-Neural-Radiance-Fields"><a href="#4-3-2-Controllable-Dynamic-Neural-Radiance-Fields" class="headerlink" title="4.3.2. Controllable Dynamic Neural Radiance Fields"></a>4.3.2. Controllable Dynamic Neural Radiance Fields</h3><p>为了使神经辐射场的变形具有可控性，该方法使用特定类别的运动模型作为变形状态的基本代表（例如，人脸的可变形模型或人体的骨骼变形图）。</p>
<p>NeRFace[GTZN21]是第一个使用可变形模型来隐式控制神经辐射场的方法（见图10）。他们使用人脸跟踪器[TZS∗16]来重建人脸混合形状参数以及训练视图（单眼视频）中的摄像机姿势。MLP是在这些视图上用混合形状参数和一个可学习的每帧潜伏代码进行训练的。此外，他们假设有一个已知的静态背景，这样辐射度场就只储存了关于面部的信息。潜伏代码被用来补偿缺失的跟踪信息（即人的肩膀）以及跟踪中的错误）。一旦经过训练，辐射场就可以通过混合形状的参数来控制，因此，允许重演和表情编辑。虽然NeRFace使用了基于可变形模型的全局变形代码，但Wang等人[WBL∗20]生成了局部动画代码。具体来说，他们从多视图输入中提取一个全局动画代码，并使用三维卷积神经网络将其映射到局部代码。这些代码被用来调节精细级别的光芒场，这些光芒场被表示为MLPs。与NeRFace相比，该方法不允许直接控制人脸的表达，但必须训练一个编码器，例如可以从面部关键点生成动画代码。Guo etal. [GCL∗21]提出了一个音频驱动的神经辐射场（AD-NeRF），其灵感来自NeRFace。但他们没有使用表达系数，而是将使用Deep-Speech[HCC∗14, TET∗20]提取的音频特征映射为一个特征，作为代表辐射场的MLP的一个条件。虽然表情是通过音频信号隐性控制的，但他们对头部的刚性姿势提供了明确的控制。为了合成一个人的肖像视图，他们采用了两个独立的辐射场，一个用于头部，一个用于躯干。</p>
<p>虽然上述方法在人像场景中显示了有希望的结果，但它们不适用于高度非刚性的变形，特别是对于从单一视图中捕获的人体关节运动。因此，方法明确地利用了胡人骨架嵌入。神经铰接辐射场（NARF）[NSLH21]通过姿势标注的图像进行训练。一个有关节的物体被分解成几个刚性的物体部分，在上面有它们的局部坐标系和全局形状变化。收敛的NARF可以通过操纵姿势、估计深度图和执行身体部分的分离来渲染新的视图。与NARF相反，A-NeRF[SYZR21]以一种自我监督的方式从单眼录像中学习特定演员的体积神经身体模型。该方法将动态NeRF体积与铰接式人体骨架嵌入的明确可控性结合起来，并以一种通过合成分析的方式重建姿势和辐射场。一旦经过训练，辐射场就可用于新的视点合成以及运动定位。在Human3.6M数据集[IPOS14]上，他们展示了使用无表面学习模型的好处，该模型在光度重建损失的帮助下，提高了单眼视频中人类姿势估计的准确度。A-NeRF是在单眼视频上训练的，而Animatable Neural Radiance Fields(ANRF)[PDW∗21]则是一种骨架驱动的方法，用于从多视角视频中重建人类模型。它的核心部分是一个新的模型表示，即。与几个一般的非刚性NeRFs[PSB∗21, TTG∗21]类似，ANRF保持了一个典型的空间，并估计了多视角输入和典型框架之间的双向对应关系。重建的可动画的人类模型可用于自由视角渲染和新姿势下的重新渲染。通过在离散的典型空间点上运行体积密度的行进方程，也可以从ANRF中提取人体模型。该方法对所学的人体模型达到了很高的视觉精度，作者建议在未来的工作中可以改进对观察到的表面上复杂的非刚性变形的处理（例如那些由于宽松的衣服引起的变形）。</p>
<p>Peng及其同事[PZX∗21]的Neural Body方法能够从稀疏的多视图视频（例如，只有四个同步视图）中对人类表演进行新的视图合成，见图11的示例性输入和结果。他们的方法使用参数化的人类形状模型SMPL[LMR∗15]作为形状代理先验。它假定在不同的帧上恢复的神经表现具有相同的锚定在可变形网格上的潜在代码集。通用基线，如刚性NeRF[MST∗20]（按时间戳应用）或NeuralVol-umes[LSS∗19]，假定输入图像集更加密集，因此，在从少数同步输入图像中演绎移动人类的新观点的能力方面，无法与Neural Body竞争。该方法与PIFuHD[SSSJ20]等人体网状结构重建技术相比也很有优势，后者在三维重建人体外观细节（例如，很少穿的或独特的服装）时，强烈依赖于训练的三维数据。与神经身体方法类似，神经演员（NA）[LHR∗21]使用SMPL模型来表示变形状态。他们利用代理来明确地将周围的三维空间解压缩成一个典型的姿势，其中嵌入了NeRF。为了改善几何和外观的高保真细节的恢复，他们使用了定义在SMPL表面的额外的二维纹理图，这些纹理图被用作NeRF MLP的额外条件。H-NeRF[XAS21]是另一种使用人体模型进行调节的人类时空三维重建技术。与Neural Body[PZX∗21]类似，他们需要一组来自同步和校准的摄像机的稀疏视频。与之相反，H-NeRF使用一个结构化的隐性身体模型，并带有签名的dis-tance字段[AXS21]，这导致了更清晰的渲染和更完整的几何形状，以应对挑战性的主题。</p>
<p>体积基元的混合[LSS∗21]是一个用于实时重塑动态、可动画的虚拟人的模型。其主要思想是用一组可以动态改变位置和内容的体积基元来模拟一个场景或物体。这些基元对场景的组成部分进行建模，就像一个基于部件的模型。每个体积计量基元是一个由解码器网络从潜伏代码中产生的体素网格。代码定义了场景的配置（例如，在人脸的情况下，面部表情），解码器网络使用它来产生基元位置和体素值（包含RGB颜色和不透明度）。渲染时，采用光线行进程序，沿每个像素对应的光线累积颜色和不透明度值。与其他动态NeRF方法类似，多视角视频被用作训练数据。该方法能够创造出极高质量的实时渲染，即使是在具有挑战性的材料上，如头发和衣服，看起来也很真实。</p>
<h2 id="4-4-Compositionality-and-Editing"><a href="#4-4-Compositionality-and-Editing" class="headerlink" title="4.4. Compositionality and Editing"></a>4.4. Compositionality and Editing</h2><p>到目前为止，所讨论的方法允许重建静态或动态场景的体积再现，并渲染其新的视图，也许是从一些输入的图像。除了比较直接的修改（例如，去除前景），它们保持观察到的场景不变。最近的一些方法还允许对重建的三维场景进行编辑，即对物体进行重新排列和仿生变换，改变其结构和外观。在表4中，我们对所讨论的方法进行了概述。</p>
<p>有条件的NeRF[LZZ∗21]可以改变在二维图像中观察到的模糊物体的颜色和形状，这些物体是由用户手动编辑的（例如，有可能删除一些物体的部分）。这一功能是由一个在同一类别的多个物体实例上训练的单一NeRF实现的。在编辑过程中，网络参数被调整以匹配新观察到的实例的形状和颜色。这项工作的贡献之一是找到了一个可调整参数的子集，它可以成功地传播用户的编辑，以实现新的视图生成。这就避免了对整个网络进行昂贵的修改。 CodeNeRF [JA21]表示整个物体类别的形状和纹理变化。与pixelNeRF类似，CodeNeRF可以合成未见过的物体的新视图。它为形状和纹理学习了两种不同的嵌入。在测试时，它从一张图像中估计出相机的姿势、物体的三维形状和纹理，并且可以通过改变它们的潜伏代码来不断修改它们。Co-deNeRF实现了与以前的单幅图像三维重建方法相媲美的性能，同时不假定已知的相机姿势。</p>
<p>神经场景图（NSG）[OMT∗21]是一种最新的方法，用于从驾驶时记录的单眼视频（自我车辆视图）合成新的视图。这种技术将具有多个独立的刚性移动物体的动态场景分解为一个学习过的场景图，该图编码了各个物体的变换和辐射。因此，每个物体和背景都由不同的神经网络来编码。此外，静态节点的采样被限制在分层平面（与图像平面平行），以提高效率，即2.5D表示。NSG重新要求在输入帧的集合上对每个刚性移动的感兴趣的物体进行注释的跟踪数据，每个物体类别（如汽车或公共汽车）共享一个体积先验。神经场景图可以用来呈现相同（即观察到的）、不同（即通过重新排列物体）场景的新观点。NSG的应用包括背景-前景分解，丰富汽车感知的训练数据集，以及改进物体检测和场景理解（见图12）。</p>
<p>Zhang等人[JXX∗21]介绍了另一种可编辑的自由视点视频的分层表示。他们的空间和时间一致的NeRF（ST-NeRF）依赖于所有独立移动和衔接的物体的边界框，从而产生了多个层次，并将它们的位置、变形和外观分解开来。ST-NeRF的输入是一组16个同步视频，这些视频来自半圆内以固定间隔放置的摄像机，以及人类背景的分割面具。该方法的名称表明，时空一致性约束反映在其结构中，即作为一个时空变形模块和一个典型空间的NeRF模块。ST-NeRF还接受时间戳，以说明外观在时间上的演变。在渲染新的视图时，取样射线会通过多个场景层投射，这导致了密度和颜色的累积。ST-NeRF可用于神经场景的编辑，如重新调整比例、移位、对表演者进行二次拼接或移除，以及时间上的重新安排。作为未来工作的有希望的方向，作者提到了减少输入视图的数量和实现非刚性的场景编辑。</p>
<p>请注意，第4.2节中讨论的一些方法[NG21b, NPLT∗19]也可以用于场景编辑。例如，GIFARRE可以旋转一个在单幅图像中观察到的已知类别的物体，改变其外观并沿深度通道进行平移。本节讨论的方法的比较见表4。</p>
<h2 id="4-5-Relighting-and-Material-Editing"><a href="#4-5-Relighting-and-Material-Editing" class="headerlink" title="4.5. Relighting and Material Editing"></a>4.5. Relighting and Material Editing</h2><p>到目前为止，我们所介绍的应用都是基于第3.2.2节中讨论的简化吸收-发射体积渲染模型，其中场景被建模为一个阻挡和发射光线的粒子体。虽然这个模型足以从新的视角渲染场景的图像，但它无法在不同的照明条件下渲染场景的图像。启用重新照明需要一个能够模拟光线在体积中传输的场景表示，包括具有各种材料特性的粒子对光线的散射。在表5中，我们对所讨论的方法进行了概述。</p>
<p>神经反射场[BXS∗20]提出了NeRF的第一个扩展，以实现再照明。在NeRF中，神经反射场不是将场景表示为一个体积密度和与视图相关的发射辐射的场，而是将场景表示为一个体积密度、表面法线和双向反射分布函数（BRDFs）的场。这允许在任意的照明条件下渲染场景，使用预测的表面法线和每个三维位置的BRDFs来评估该位置的粒子向摄像机反射了多少入射光。然而，对于神经体积渲染模型来说，评估从摄像机射线上的每一点到每一个光源的可见度是非常耗费计算的。即使只是考虑直接光照，MLP也必须在沿摄像机光线的每一点和每个光源之间的密集采样位置进行评估，以便计算出入射光线的光照。神经反射场避免了这一问题，它只用与摄像机同处一地的单一点光源照亮的物体图像进行训练，因此MLP只需要沿摄像机光线进行评估。</p>
<p>最近其他恢复可重燃模型的工作，通过简单地忽略自我排除和假设任何表面以上的上半球中的所有光源都是完全可见的，从而避免了计算光源可见性的困难。PhySG[ZLW∗21]和NeRD[BBJ∗21]都假设光源完全可见，并通过将环境照明和场景BRDFs表示为球面高斯方的混合物来进一步加速渲染，这使得入射光线乘以BRDF的半球积分可以以封闭形式计算。假设光源完全可见，对于主要是凸面的物体来说效果很好，但这种策略无法模拟诸如由于光源被场景几何形状遮挡而产生的投射阴影等效果。</p>
<p>神经反射和可见度场[SDZ∗21]（NeRV）训练一个MLP，以接近任何输入的3D位置和2D入射光线方向的光源可见度。与沿每条光线密集采样的点查询MLP不同，可见度MLP只需要对每个入射光线方向进行一次查询。这使得NeRV能够从具有明显阴影和自闭效应的图像中恢复可重照的场景模型。</p>
<p>NeRFactor[ZSD∗21]不是像前面讨论的方法那样从头开始优化可重现的表示，而是从一个预先训练好的NeRF模型开始。然后，NeRFactor通过将预训练的NeRF的体积几何简化为表面模型，优化MLP来代表光源的可见性和表面任意点的法线，最后优化环境照明和表面任意点的BRDF的表示，来恢复可照明模型；见图13的分解示例。这就产生了一个可重现的模型，在渲染图像时效率更高，因为体积几何学已经被简化为一个单一的表面，任何一点的光源可见度都可以通过一个MLP查询来计算。</p>
<p>上面描述的可重燃模型将场景母体表现为连续的三维BRDFs场。这使得一些基本的材料编辑成为可能，因为恢复的BRDFs可以在渲染前进行改变。NeuTex[XXH∗21]通过引入一个表面参数化网络工作，学习从体积中的三维坐标到二维纹理坐标的映射，实现了更直观的材料编辑。在一个场景的NeuTex模型被恢复后，二维纹理可以很容易地被编辑或替换。</p>
<h2 id="4-6-Engineering-Frameworks"><a href="#4-6-Engineering-Frameworks" class="headerlink" title="4.6. Engineering Frameworks"></a>4.6. Engineering Frameworks</h2><p>对于从业者来说，使用神经渲染模型的工作带来了工程上的挑战：大量的图像和视频数据必须以高度非连续的方式进行处理，而且这些模型往往需要对大型复杂的计算图进行区分。在本节中，我们将讨论有助于克服这些问题的工具的最新进展。</p>
<h3 id="4-6-1-Storage"><a href="#4-6-1-Storage" class="headerlink" title="4.6.1. Storage"></a>4.6.1. Storage</h3><p>用数据饱和GPU，特别是用于神经渲染的数据是很有挑战性的：通常，图像或视频的每个像素都被当作一个独立的数据点。方法需要在整个数据集的像素池上进行随机迭代，在时间重建的情况下，往往需要在单个批次的整个序列中进行迭代。灵活的存储解决方案应该考虑到这一点。</p>
<p>英伟达AIStore[AMB19]是一个通用的存储解决方案，它允许监控每个驱动器的吞吐量，并实现了加载和洗牌的分层架构，同时将这些分层从用户那里抽象出来。独立于存储后端，分片存储有巨大的好处：1）允许在内存中洗数据，同时2）在分片内主要使用顺序读取。Tensorflow[AAB∗15]通过tfrecord文件格式内置支持分片存储，而webdataset Woff为PyTorch[PGM∗19]提供类似的便利功能。</p>
<h3 id="4-6-2-Hyperparameter-Search-and-Experiments"><a href="#4-6-2-Hyperparameter-Search-and-Experiments" class="headerlink" title="4.6.2. Hyperparameter Search and Experiments"></a>4.6.2. Hyperparameter Search and Experiments</h3><p>由于运行时间长，配置层次复杂，神经网络渲染实验需要良好的实验管理和超参数搜索技术。Hydra[Yad19]擅长配置最复杂的实验，并为超参数搜索提供集成支持，例如使用AX自适应实验框架W。然而，运行等效实验进行扫描直到收敛，即使是使用贝叶斯超参数搜索巧妙地挑选参数，也可能耗费时间。Ray tune[LLN∗18]有ASHA[LJR∗20]和Hyperband[LJRT18]等算法的实现，这些算法可以将计算和时间预算分配给实验，以加快超参数搜索。</p>
<h3 id="4-6-3-Differentiable-Rendering-and-Autodiff"><a href="#4-6-3-Differentiable-Rendering-and-Autodiff" class="headerlink" title="4.6.3. Differentiable Rendering and Autodiff"></a>4.6.3. Differentiable Rendering and Autodiff</h3><p>神经渲染对可区分性有很高的要求：需要建立复杂的计算图，并且根据应用，要么对大量输入进行矢量处理（macroAD-为了简洁起见，我们在本节中将自动区分称为AD），要么对大量的小输入进行处理（micro AD）。根据应用的不同，AD包可能要用低级别的（比如在CUDA中），也可能用高级别的（比如在Python中）。一个功能强大的C++ AD库是STAN [CHB∗15]。我们可以参考随附的论文，以了解对AD库的全面概述和评估，直到它在2015年出版，这超出了本文的范围。一个值得注意的最近的C++17的AD包是autodiff Wpackage。Enzyme AD [MC]采用了一种特别通用的低级AD方法：它利用了整个LLVMecosystem。这特别强大，因为前台、LLVM IR和后台的概念。概括地说，LLVM前台将一种语言，例如C++，翻译成LLVM中间表示法（IR）。这种表示法是一种抽象的、与语言无关的低级命令表示法，它对所有前台都是一样的。这就是Enzyme的作用：它是一个扩展，可以在这个IR中创建函数的衍生物。这意味着它适用于LLVM支持的所有语言。LLVM后端从IR中发射代码：这可能是针对x86、ARMor GPU处理器的。这意味着，Enzyme支持各种处理器，包括GPU。另一个专门用于处理图像和图形的C++包是Halide [LGA∗18]。它的突出特点是对像素的并行处理进行灵活的调度。</p>
<p>Difftaichi [HAL∗20]在Python中提供了用于物理模拟的可微分编程，并应用于渲染。Enoki [Jak19]是一个非常通用的高性能AD组件，用于基于物理的可微分渲染，是Mitsuba 2渲染器的核心组件[NDVZJ19]。Jax [BFH∗18]是一个用于可微分和加速线性代数的Python框架，具有针对GPU和TPU的编译选项。JaxNeRF是一个使用Jax的NeRF参考实现。Swift编程语言提供了AD作为第一类用例W，并被大量用于开发Tensorflow的集成。</p>
<h3 id="4-6-4-Raycasting-and-Rendering"><a href="#4-6-4-Raycasting-and-Rendering" class="headerlink" title="4.6.4. Raycasting and Rendering"></a>4.6.4. Raycasting and Rendering</h3><p>目前有几个软件包可以提供高级渲染和聚合基元。NVIDIA OptiX Wis是一个用于光线投射和光线相交的高性能库，并提供了迄今为止在NVIDIARTX硬件上使用硬件加速进行光线相交的唯一可能性。Teg[BMM∗21]是一种可区别的编程语言，它提供了在渲染中经常发现的不连续积分的操作原语。Redner[LADL18b]是一个用于微分跟踪的框架；Mitsuba 2[NDVZJ19]为基于物理的微分渲染和路径跟踪提供了一个更全面的框架。 PyTorch3D [RRN∗20]提供了一套广泛的围绕可微分渲染和图形的工具，与PyTorch紧密结合。Tensorflow Graphics [VKP∗19]对Tensorflow有类似的目标。</p>
<h1 id="5-Open-Challenges"><a href="#5-Open-Challenges" class="headerlink" title="5. Open Challenges"></a>5. Open Challenges</h1><p>在涵盖了神经体积表征可以成功应用的各种计算机图形和视觉问题之后，我们现在来看看那些只使用了经典表征的问题。因此，未来的研究有各种途径。我们进一步讨论该领域的多个开放性挑战。下面讨论的许多观点是相互关联的。</p>
<p><strong>无缝集成和使用</strong>。半个多世纪以来，大多数计算机图形算法和技术都是以网格或点云作为三维场景的表现形式进行渲染和编辑。相比之下，神经渲染是一个非常年轻的领域，这个概念在几年前的2018年才首次被使用[ERB∗18]。因此，不可避免的是，在可以操作经典三维表现的现有方法谱系和适用于神经表现的方法谱系之间仍然存在差距。此外，许多方法存在于编辑经典表现，例如，广泛使用的工具，如Blender[Com18]和Maya[Aut]支持网格和纹理图，而他们的对应神经表现必须从头开发。另一方面，可以预见的是，随着该领域的进一步改进以及神经表征的越来越广泛的采用和整合，这种差距将会缩小。此外，现代硬软件加速器是为经典的计算机图形设计的，将来也可以为神经表征进行类似的定制。</p>
<p>另一个相关的挑战是学习到的表征的可解释性，这涉及到一般的深度学习。因此，学习到的神经网络权重很难用目标量（例如三维空间中的点颜色和不透明度）来解释。同时，他们的目标是取代图形管道，而图形管道是很好理解的，并依赖于分析得出的步骤。</p>
<p>最终，为了提高可控性，并使计算机图形工具中学习到的体积模型能够无缝整合，我们希望能够修改场景参数，使场景向所需方向变化。虽然这对于由全局MLPs参数化的任意场景来说很可能是不可行的，但由局部神经表征组成一个完整的场景可能会使其具有可操作性，为重新引入经典图形的各个方面提供了有趣的可能性。</p>
<p><strong>可扩展性</strong>。大多数关于体积神经渲染的工作都是针对单个物体和相对简单的复合场景（例如，一个人和一个背景，几个人在同一个环境中，一条有移动的汽车的街道），有的没有背景。学习大规模场景的神经表征–这些场景只能在每个输入帧中部分观察到–仍然是一个挑战。尽管在这个方向上已经迈出了成功和令人印象深刻的第一步（我们在这里指的是Nerf in the Wild[MBRS∗21]和神经SLAM系统iMAP[SLOD21]，见图14），但仍有许多公开的挑战。例如，为单个物体开发的场景编辑、重新打光和合成的方法不能直接扩展到处理大规模场景。此外，从某种场景大小开始，大规模环境的全局表示就变得不可行了，即使在应用空间分割策略时，如PlenOctrees[YLT∗21]中使用的策略。因此，需要为大规模场景的高效神经模型开发新一代的存储和重新检索技术，其思路与用于TSDF的VoxelHashing[NZIS13]类似。首先，它们应该使场景的完成更加有效（也就是说，不需要不断地从头开始重新计算整个模型），其次，使部分内容的检索变得容易。这两点都与上面讨论的可解释性的公开挑战有关。</p>
<p><strong>可通用性</strong>。只有少数初步但有希望的方法存在于可泛化和可实例化的体积神经代表中。例如，StereoNeRF[CBLPM21]仅使用十几个分散的视图来生成刚性场景的新视图，其视觉精度在微调后与原始NeRF[MST∗20]相当，而pixelNeRF[YYTK21]仅从一张图像就能推断出训练时未看到的刚性场景的体积模型。这类方法是数据驱动的，需要有足够宽的基线的大规模多视图数据集。因此，如果数据集提供足够的视点覆盖，这些方法可以在任意的新视点上产生视图。减少这种强烈的依赖性是未来工作的一个令人兴奋的方向。另一个开放的挑战是可实例化的方法对非刚性变形的场景的通用性。输入可以是稀疏的时空观测数据集，甚至是极端的单一图像（在这种情况下，任务变成了从单一图像出发的场景动画）。实现这种技术的一个直接方向是依靠可变形场景的多视图数据集，这可能会使所需的数据集大小增加很多。另一个可能的方法是将变形模式和场景的形状以及静止时的外观分开。此外，虽然有一些关于生成神经场景表征的工作（例如，使用超网络[SRF∗21]），但在设计以神经场景表征为输入的神经操作者方面进展较少，例如，完成一个部分场景或为现有表征添加语义标签。目前还没有类似于网格卷积的运算器，也没有类似于三维卷积的运算器。这样的算子最好只训练一次，然后就能普遍适用。</p>
<p><strong>多模式学习</strong>。多模态学习意味着超越视觉信号，并纳入其他数据类型，如图像学、文本描述和声音。例如，远程呈现和增强现实将从一种方法中获益匪浅，这种方法不仅可以呈现动态互动和交谈的人类的新观点，而且还可以合成相应的新声音；例如，现有的工作可以从单声道音频输入中合成立体声音频[RMG∗21]。合成文本描述和场景的语义（例如，语义分割la-bels）对于基于体积神经表征的下游应用非常有用。虽然之前的一些工作解决了这个目标[KSW20,ZLLD21]，但这仍然是一个开放的挑战。更详细的和针对特定场景的建模可以考虑到相机捕获系统（例如，如TöRF[ALG∗21]中已经显示的深度相机）、相机是否使用滚动或全局快门，或者输入图像中是否存在运动模糊等信息。其他传感器如IMU、激光雷达或事件流都有可能以连续的方式进行建模。(超声波和X射线可以以任意的分辨率连续建模，用于医学成像。） 还可以设想对某些不容易测量的捕捉属性进行优化，如多视角捕捉装置的颜色校准。这延伸到一般的物理模拟，其中神经场景表示提供了一个令人兴奋的场所，通过纳入可区别的物理模拟器来 “少学多知”；例如，用于物理动机的去形模型或物理上正确的光传输。</p>
<p>其他问题。我们能提高质量吗？重建具有许多高频细节、阴影和视线依赖性外观的物体仍然是一个基本未解决的问题。我们能减少训练时间吗？尽管在测试时对新的视图合成的快速推断方面已经取得了进展，但改善训练时间仍然是一个很大的挑战。较少的输入图像是否足够？</p>
<p>较少的输入图像可能足以达到与需要数百张图像的完全收敛模型类似的视觉效果。目前，部分观察（例如，只在图像的子集中观察到的场景的一部分）往往比场景的其他部分更模糊。</p>
<p>除了AR/VR的直接使用案例，在其他背景下使用神经场景表征的研究很少，比如机器人，但实时SLAM系统iMAP[SLOD21]是个明显的例外（见图14）。我们怎样才能获得、纳入和预测物体的承受力或其他注释，如温度？使用神经场景表征进行运动预测或规划是否有优势？</p>
<p>本节所讨论的未来方向的清单并不是为了完整。我们希望在不久的将来看到基于坐标的神经体积表征在更多方面的改进。</p>
<h1 id="6-Social-Implications"><a href="#6-Social-Implications" class="headerlink" title="6. Social Implications"></a>6. Social Implications</h1><p>在这份最先进的报告中讨论的神经方法对合成的新观点实现了非常高的真实性。该领域的快速发展已经影响并将继续以许多积极和潜在的消极方式影响社会，我们将在本节中讨论。</p>
<p><strong>研究和工业</strong>。受到新的体积神经表征明显影响的领域是计算机视觉、计算机图形以及增强和虚拟现实，它们可以从渲染环境的照片真实性的提高中受益。事实上，最先进的体积计量模型依赖于被充分理解和优雅的原理，这降低了摄影测量和三维重建研究的门槛。此外，这些方法的易用性以及公开的代码库和数据集也放大了这种效果。由于神经渲染仍未成熟和被充分理解，像Blender这样的终端用户工具还不存在，这使得这些新方法目前对3D爱好者和工业界来说都遥不可及。然而，对该技术更广泛的了解必然会影响到开发的产品和应用。因此，我们可以预见，在游戏内容创作和电影特效方面的努力将会减少。与现有的技术相比，从少数输入的图像中呈现照片般逼真的新场景的可能性是一个显著的优势。这有可能重塑视觉效果（VFX）行业中使用的整个既定的内容设计管道。</p>
<p><strong>可信度</strong>。然而，与此同时，照片的真实性也带来了滥用技术的可能性，创造出的合成内容可能会被恶意的行为者误认为是真实的，特别是当神经渲染方法集中在人脸时[TZS∗16, TZN19, GTZN21]。为了应对这些潜在的滥用，研究界正在开发自动检测这种虚假内容的方法[CRT∗21, RCV∗19]，并且正在探索包括加密和区块链措施在内的安全措施。还有其他一些可以探索的缓解措施，以尽量减少这些风险。例如，在有些情况下，我们希望用户不反对看到合成的真实照片内容（例如，在看电影时），但合成内容可以被标记或以其他方式识别出来，以告知用户。进一步的用户研究可以调查人们在不同情况下对标记合成内容的必要性的判断。在收集方面，人们可以提供明确和知情的同意，他们的身份可以被用来在特定的环境中创建合成内容。</p>
<p><strong>环境</strong>。由于目前的神经体积场景代表是基于深度学习的，用于训练它们的GPU会消耗大量的能量。由于越来越多的实验室正在研究神经渲染，高端和多GPU系统的使用也相应增加。如果制造资源和操作GPU集群的电力不是主要来自可再生资源，那么从长远来看，训练大量的神经表征会对环境和全球气候产生负面影响。为了减轻对计算资源的需求，从而减少电力和硬件，有许多架构需要比基于NeRF的方法更少的计算能力来进行训练。最后但并非最不重要的是，高的GPU需求可能意味着并非所有的团体都能负担得起平等的贡献，因为实验体积表达不是最轻便的任务。</p>
<h1 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h1><p>在这份最先进的报告中，我们回顾了最近关于神经渲染技术的趋势。这些方法基于二维观察作为训练的输入，学习三维神经场景表征，并通过对不同场景参数的控制来合成照片般真实的图像。在过去的几年里，神经渲染领域取得了快速的进展，并继续快速增长。其应用范围包括从刚性和非刚性场景的自由视角视频到形状和材料编辑、重新照明和人类化身生成等。本报告对这些应用进行了详细讨论。</p>
<p>同时，我们认为神经渲染仍然是一个新兴的领域，有许多开放的挑战可以解决。此外，我们还讨论了社会影响，这些影响来自于神经渲染的民主化，以及它合成照片般真实的图像内容的能力。总的来说，我们的结论是，神经渲染是一个令人兴奋的领域，它激励着成千上万的研究人员在许多社区解决计算机图形学中最难的问题，我们期待着看到该主题的进一步发展。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a>
                    
                  </div>
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'YoujiaZhang/commit-Utterances');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>hexo</span></a> <i class="iconfont icon-plan"></i> <a href="https://YoujiaZhang.github.io" target="_blank" rel="nofollow noopener"><span>Yj-Zhang</span></a> <i class="iconfont icon-love"></i> <a target="_blank" rel="nofollow noopener"><span>D.H.</span></a> <div style="font-size: 0.85rem"> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/js/duration.js"></script> </div> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  





  <script  src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js" ></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({"theme":"default"});
    }
  </script>




  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?siteId=17279802";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
